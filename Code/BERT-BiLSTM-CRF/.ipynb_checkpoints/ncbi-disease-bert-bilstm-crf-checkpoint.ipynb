{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'F:/BioNER-Abbrev/Dataset/NCBI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T11:50:21.214839Z",
     "iopub.status.busy": "2023-12-04T11:50:21.214570Z",
     "iopub.status.idle": "2023-12-04T11:51:00.627797Z",
     "shell.execute_reply": "2023-12-04T11:51:00.627001Z",
     "shell.execute_reply.started": "2023-12-04T11:50:21.214809Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_name = \"athiban2001/cord-scibert\"\n",
    "\n",
    "pubmedbert = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast = False)\n",
    "pubmedbert.save_pretrained(f\"./pretrained_model/pubmedbert\")\n",
    "tokenizer.save_pretrained(\"./pretrained_model/pubmedbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T11:51:00.629054Z",
     "iopub.status.busy": "2023-12-04T11:51:00.628858Z",
     "iopub.status.idle": "2023-12-04T11:51:09.771169Z",
     "shell.execute_reply": "2023-12-04T11:51:09.770399Z",
     "shell.execute_reply.started": "2023-12-04T11:51:00.629030Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "NUM_WORKER = 5\n",
    "BASE_MODEL_PATH = './pretrained_model/pubmedbert'\n",
    "MODEL_PATH = \"models/NBCI_pubmedbert\"\n",
    "TRAINING_FILE = 'F:/BioNER-Abbrev/Dataset/JNLPBA'\n",
    "TOKENIZER = transformers.BertTokenizer.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    do_lower_case=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T11:51:09.773240Z",
     "iopub.status.busy": "2023-12-04T11:51:09.773007Z",
     "iopub.status.idle": "2023-12-04T11:51:09.786223Z",
     "shell.execute_reply": "2023-12-04T11:51:09.785230Z",
     "shell.execute_reply.started": "2023-12-04T11:51:09.773211Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class EntityDataset:\n",
    "    def __init__(self, texts, tags,enc_tag):\n",
    "        # texts: [[\"hi\", \",\", \"my\", \"name\", \"is\", \"abhishek\"], [\"hello\".....]]\n",
    "        # pos/tags: [[1 2 3 4 1 5], [....].....]]\n",
    "        self.texts = texts\n",
    "        # self.pos = pos\n",
    "        self.tags = tags\n",
    "        self.enc_tag=enc_tag\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item]\n",
    "        # pos = self.pos[item]\n",
    "        tags = self.tags[item]\n",
    "\n",
    "        ids = []\n",
    "        # target_pos = []\n",
    "        target_tag =[]\n",
    "\n",
    "        for i, s in enumerate(text):\n",
    "            inputs = TOKENIZER.encode(\n",
    "                str(s),\n",
    "                add_special_tokens=False\n",
    "            )\n",
    "            # abhishek: ab ##hi ##sh ##ek\n",
    "            input_len = len(inputs)\n",
    "            ids.extend(inputs)\n",
    "            # target_pos.extend([pos[i]] * input_len)\n",
    "            target_tag.extend([tags[i]] * input_len)\n",
    "\n",
    "        ids = ids[:MAX_LEN - 2]\n",
    "        # target_pos = target_pos[:MAX_LEN - 2]\n",
    "        target_tag = target_tag[:MAX_LEN - 2]\n",
    "\n",
    "        ids = [102] + ids + [103]\n",
    "        # target_pos = [0] + target_pos + [0]\n",
    "        o_tag=self.enc_tag.transform([\"O\"])[0]\n",
    "        target_tag = [o_tag] + target_tag + [o_tag]\n",
    "\n",
    "        mask = [1] * len(ids)\n",
    "        token_type_ids = [0] * len(ids)\n",
    "\n",
    "        padding_len = MAX_LEN - len(ids)\n",
    "\n",
    "        ids = ids + ([0] * padding_len)\n",
    "        mask = mask + ([0] * padding_len)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
    "        # target_pos = target_pos + ([0] * padding_len)\n",
    "        target_tag = target_tag + ([0] * padding_len)\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            # \"target_pos\": torch.tensor(target_pos, dtype=torch.long),\n",
    "            \"target_tag\": torch.tensor(target_tag, dtype=torch.long),\n",
    "            # \"words\":torch.tensor(words,dtype=torch.int)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T11:51:09.787582Z",
     "iopub.status.busy": "2023-12-04T11:51:09.787324Z",
     "iopub.status.idle": "2023-12-04T11:51:09.802252Z",
     "shell.execute_reply": "2023-12-04T11:51:09.801585Z",
     "shell.execute_reply.started": "2023-12-04T11:51:09.787546Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "\n",
    "\n",
    "class EntityModel(nn.Module):\n",
    "    def __init__(self, num_tag):\n",
    "        super(EntityModel, self).__init__()\n",
    "        self.num_tag = num_tag\n",
    "        self.bert = transformers.BertModel.from_pretrained(BASE_MODEL_PATH,return_dict=False)\n",
    "        self.bilstm= nn.LSTM(768, 1024 // 2, num_layers=1, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.dropout_tag = nn.Dropout(0.3)\n",
    "        \n",
    "        self.hidden2tag_tag = nn.Linear(1024, self.num_tag)\n",
    "\n",
    "        self.crf_tag = CRF(self.num_tag, batch_first=True)\n",
    "    \n",
    "    \n",
    "    # return the loss only, not encode the tag\n",
    "    def forward(self, ids, mask, token_type_ids, target_tag):\n",
    "        x, _ = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        h, _ = self.bilstm(x)\n",
    "\n",
    "        o_tag = self.dropout_tag(h)\n",
    "        tag = self.hidden2tag_tag(o_tag)\n",
    "        mask = torch.where(mask==1, True, False)\n",
    "\n",
    "        loss_tag = - self.crf_tag(tag, target_tag, mask=mask, reduction='token_mean')\n",
    "        loss=loss_tag\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "    # encode the tag, dont return loss\n",
    "    def encode(self, ids, mask, token_type_ids, target_tag):\n",
    "        # Bert - BiLSTM\n",
    "        x, _ = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        h, _ = self.bilstm(x)\n",
    "\n",
    "        # drop out\n",
    "        o_tag = self.dropout_tag(h)\n",
    "        # o_pos = self.dropout_pos(h)\n",
    "\n",
    "        # Hidden2Tag (Linear)\n",
    "        tag = self.hidden2tag_tag(o_tag)\n",
    "        mask = torch.where(mask==1, True, False)\n",
    "        tag = self.crf_tag.decode(tag, mask=mask)\n",
    "\n",
    "        return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T11:51:09.803760Z",
     "iopub.status.busy": "2023-12-04T11:51:09.803399Z",
     "iopub.status.idle": "2023-12-04T11:51:10.142396Z",
     "shell.execute_reply": "2023-12-04T11:51:10.141777Z",
     "shell.execute_reply.started": "2023-12-04T11:51:09.803722Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import joblib\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "\n",
    "total_tags=[]\n",
    "with open(data_folder + \"/classes.txt\") as f:\n",
    "  for line in f.readlines():\n",
    "    total_tags.append(line.strip())\n",
    "\n",
    "enc_tag = preprocessing.LabelEncoder()\n",
    "enc_tag.fit(list(total_tags))\n",
    "\n",
    "def process_data(data_path):\n",
    "    sentences,tags=[],[]\n",
    "    sentence,tag=[],[]\n",
    "    \n",
    "    total_tags=set()\n",
    "    i=0\n",
    "    \n",
    "    for path in data_path:\n",
    "        with open(path,\"r\") as f:\n",
    "            for line in f:\n",
    "                if i%10000==0:\n",
    "                  print(len(sentences))\n",
    "                i+=1\n",
    "                line=line.strip()\n",
    "                if line.startswith(\"-DOCSTART-\"):\n",
    "                    continue\n",
    "                elif len(line)==0:\n",
    "                    if sentence==[] and tag==[]:\n",
    "                        continue\n",
    "                    sentences.append(sentence)\n",
    "                    tags.append(tag)\n",
    "                    sentence,tag=[],[]\n",
    "                else:\n",
    "                    s,t=line.split(\"\\t\")\n",
    "                    sentence.append(s)\n",
    "                    tag.append(t)\n",
    "\n",
    "    for i in range(len(tags)):\n",
    "        tags[i]=enc_tag.transform(tags[i])\n",
    "                \n",
    "    return sentences, tags, enc_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T11:51:10.143930Z",
     "iopub.status.busy": "2023-12-04T11:51:10.143696Z",
     "iopub.status.idle": "2023-12-04T11:51:10.174362Z",
     "shell.execute_reply": "2023-12-04T11:51:10.173744Z",
     "shell.execute_reply.started": "2023-12-04T11:51:10.143900Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "def split_tag(chunk_tag):\n",
    "    \"\"\"\n",
    "    split chunk tag into IOBES prefix and chunk_type\n",
    "    e.g. \n",
    "    B-PER -> (B, PER)\n",
    "    O -> (O, None)\n",
    "    \"\"\"\n",
    "    if chunk_tag == 'O':\n",
    "        return ('O', None)\n",
    "    return chunk_tag.split(\"-\",maxsplit=1)\n",
    "\n",
    "def is_chunk_end(prev_tag, tag):\n",
    "    \"\"\"\n",
    "    check if the previous chunk ended between the previous and current word\n",
    "    e.g. \n",
    "    (B-PER, I-PER) -> False\n",
    "    (B-LOC, O)  -> True\n",
    "    Note: in case of contradicting tags, e.g. (B-PER, I-LOC)\n",
    "    this is considered as (B-PER, B-LOC)\n",
    "    \"\"\"\n",
    "    prefix1, chunk_type1 = split_tag(prev_tag)\n",
    "    prefix2, chunk_type2 = split_tag(tag)\n",
    "\n",
    "    if prefix1 == 'O':\n",
    "        return False\n",
    "    if prefix2 == 'O':\n",
    "        return prefix1 != 'O'\n",
    "\n",
    "    if chunk_type1 != chunk_type2:\n",
    "        return True\n",
    "\n",
    "    return prefix2 in ['B', 'S'] or prefix1 in ['E', 'S']\n",
    "\n",
    "def is_chunk_start(prev_tag, tag):\n",
    "    \"\"\"\n",
    "    check if a new chunk started between the previous and current word\n",
    "    \"\"\"\n",
    "    prefix1, chunk_type1 = split_tag(prev_tag)\n",
    "    prefix2, chunk_type2 = split_tag(tag)\n",
    "\n",
    "    if prefix2 == 'O':\n",
    "        return False\n",
    "    if prefix1 == 'O':\n",
    "        return prefix2 != 'O'\n",
    "\n",
    "    if chunk_type1 != chunk_type2:\n",
    "        return True\n",
    "\n",
    "    return prefix2 in ['B', 'S'] or prefix1 in ['E', 'S']\n",
    "\n",
    "\n",
    "def calc_metrics(tp, p, t, percent=True):\n",
    "    \"\"\"\n",
    "    compute overall precision, recall and FB1 (default values are 0.0)\n",
    "    if percent is True, return 100 * original decimal value\n",
    "    \"\"\"\n",
    "    precision = tp / p if p else 0\n",
    "    recall = tp / t if t else 0\n",
    "    fb1 = 2 * precision * recall / (precision + recall) if precision + recall else 0\n",
    "    if percent:\n",
    "        return 100 * precision, 100 * recall, 100 * fb1\n",
    "    else:\n",
    "        return precision, recall, fb1\n",
    "\n",
    "\n",
    "def count_chunks(true_seqs, pred_seqs):\n",
    "    \"\"\"\n",
    "    true_seqs: a list of true tags\n",
    "    pred_seqs: a list of predicted tags\n",
    "    return: \n",
    "    correct_chunks: a dict (counter), \n",
    "                    key = chunk types, \n",
    "                    value = number of correctly identified chunks per type\n",
    "    true_chunks:    a dict, number of true chunks per type\n",
    "    pred_chunks:    a dict, number of identified chunks per type\n",
    "    correct_counts, true_counts, pred_counts: similar to above, but for tags\n",
    "    \"\"\"\n",
    "    correct_chunks = defaultdict(int)\n",
    "    true_chunks = defaultdict(int)\n",
    "    pred_chunks = defaultdict(int)\n",
    "\n",
    "    correct_counts = defaultdict(int)\n",
    "    true_counts = defaultdict(int)\n",
    "    pred_counts = defaultdict(int)\n",
    "\n",
    "    prev_true_tag, prev_pred_tag = 'O', 'O'\n",
    "    correct_chunk = None\n",
    "\n",
    "    for true_tag, pred_tag in zip(true_seqs, pred_seqs):\n",
    "        if true_tag == pred_tag:\n",
    "            correct_counts[true_tag] += 1\n",
    "        true_counts[true_tag] += 1\n",
    "        pred_counts[pred_tag] += 1\n",
    "\n",
    "        _, true_type = split_tag(true_tag)\n",
    "        _, pred_type = split_tag(pred_tag)\n",
    "\n",
    "        if correct_chunk is not None:\n",
    "            true_end = is_chunk_end(prev_true_tag, true_tag)\n",
    "            pred_end = is_chunk_end(prev_pred_tag, pred_tag)\n",
    "\n",
    "            if pred_end and true_end:\n",
    "                correct_chunks[correct_chunk] += 1\n",
    "                correct_chunk = None\n",
    "            elif pred_end != true_end or true_type != pred_type:\n",
    "                correct_chunk = None\n",
    "\n",
    "        true_start = is_chunk_start(prev_true_tag, true_tag)\n",
    "        pred_start = is_chunk_start(prev_pred_tag, pred_tag)\n",
    "\n",
    "        if true_start and pred_start and true_type == pred_type:\n",
    "            correct_chunk = true_type\n",
    "        if true_start:\n",
    "            true_chunks[true_type] += 1\n",
    "        if pred_start:\n",
    "            pred_chunks[pred_type] += 1\n",
    "\n",
    "        prev_true_tag, prev_pred_tag = true_tag, pred_tag\n",
    "    if correct_chunk is not None:\n",
    "        correct_chunks[correct_chunk] += 1\n",
    "\n",
    "    return (correct_chunks, true_chunks, pred_chunks, \n",
    "        correct_counts, true_counts, pred_counts)\n",
    "\n",
    "def get_result(correct_chunks, true_chunks, pred_chunks,\n",
    "    correct_counts, true_counts, pred_counts, verbose=True):\n",
    "    \"\"\"\n",
    "    if verbose, print overall performance, as well as preformance per chunk type;\n",
    "    otherwise, simply return overall prec, rec, f1 scores\n",
    "    \"\"\"\n",
    "    # sum counts\n",
    "    sum_correct_chunks = sum(correct_chunks.values())\n",
    "    sum_true_chunks = sum(true_chunks.values())\n",
    "    sum_pred_chunks = sum(pred_chunks.values())\n",
    "\n",
    "    sum_correct_counts = sum(correct_counts.values())\n",
    "    sum_true_counts = sum(true_counts.values())\n",
    "\n",
    "    nonO_correct_counts = sum(v for k, v in correct_counts.items() if k != 'O')\n",
    "    nonO_true_counts = sum(v for k, v in true_counts.items() if k != 'O')\n",
    "\n",
    "    chunk_types = sorted(list(set(list(true_chunks) + list(pred_chunks))))\n",
    "\n",
    "    # compute overall precision, recall and FB1 (default values are 0.0)\n",
    "    prec, rec, f1 = calc_metrics(sum_correct_chunks, sum_pred_chunks, sum_true_chunks)\n",
    "    res = (prec, rec, f1)\n",
    "    if not verbose:\n",
    "        return res\n",
    "\n",
    "    # print overall performance, and performance per chunk type\n",
    "    \n",
    "    print(\"processed %i tokens with %i phrases; \" % (sum_true_counts, sum_true_chunks), end='')\n",
    "    print(\"found: %i phrases; correct: %i.\\n\" % (sum_pred_chunks, sum_correct_chunks), end='')\n",
    "        \n",
    "    print(\"accuracy: %6.2f%%; (non-O)\" % (100*nonO_correct_counts/nonO_true_counts))\n",
    "    print(\"accuracy: %6.2f%%; \" % (100*sum_correct_counts/sum_true_counts), end='')\n",
    "    print(\"precision: %6.2f%%; recall: %6.2f%%; FB1: %6.2f\" % (prec, rec, f1))\n",
    "\n",
    "    # for each chunk type, compute precision, recall and FB1 (default values are 0.0)\n",
    "    for t in chunk_types:\n",
    "        prec, rec, f1 = calc_metrics(correct_chunks[t], pred_chunks[t], true_chunks[t])\n",
    "        print(\"%17s: \" %t , end='')\n",
    "        print(\"precision: %6.2f%%; recall: %6.2f%%; FB1: %6.2f\" %\n",
    "                    (prec, rec, f1), end='')\n",
    "        print(\"  %d\" % pred_chunks[t])\n",
    "\n",
    "    return res\n",
    "    # you can generate LaTeX output for tables like in\n",
    "    # http://cnts.uia.ac.be/conll2003/ner/example.tex\n",
    "    # but I'm not implementing this\n",
    "\n",
    "def evaluate(true_seqs, pred_seqs, verbose=True):\n",
    "    (correct_chunks, true_chunks, pred_chunks,\n",
    "        correct_counts, true_counts, pred_counts) = count_chunks(true_seqs, pred_seqs)\n",
    "    result = get_result(correct_chunks, true_chunks, pred_chunks,\n",
    "        correct_counts, true_counts, pred_counts, verbose=verbose)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T11:51:10.175892Z",
     "iopub.status.busy": "2023-12-04T11:51:10.175415Z",
     "iopub.status.idle": "2023-12-04T11:51:10.190164Z",
     "shell.execute_reply": "2023-12-04T11:51:10.189569Z",
     "shell.execute_reply.started": "2023-12-04T11:51:10.175862Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_fn(data_loader, model, optimizer, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    for data in tqdm(data_loader, total=len(data_loader)):\n",
    "        for k, v in data.items():\n",
    "            data[k] = v.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(**data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        final_loss += loss.item()\n",
    "    return final_loss / len(data_loader)\n",
    "\n",
    "def eval_fn(data_loader, model, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "\n",
    "    for data in tqdm(data_loader, total=len(data_loader)):\n",
    "        for k, v in data.items():\n",
    "            data[k] = v.to(device)\n",
    "        loss = model(**data)\n",
    "        final_loss += loss.item()\n",
    "    return final_loss / len(data_loader)\n",
    "\n",
    "def test_fn(dataset,model,device,enc_tag):\n",
    "  final_test = []\n",
    "  final_pred = []\n",
    "  O=enc_tag.transform([\"O\"])[0]\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    for data in tqdm(dataset):\n",
    "      for k, v in data.items():\n",
    "          data[k] = v.to(device).unsqueeze(0)\n",
    "\n",
    "      tag = model.encode(**data)\n",
    "      padded_pred=tag[0]\n",
    "      test=data[\"target_tag\"].cpu()[0][:len(padded_pred)]\n",
    "      test=enc_tag.inverse_transform(test)\n",
    "      padded_pred=enc_tag.inverse_transform(padded_pred)\n",
    "      final_pred.extend(padded_pred[1:-1])\n",
    "      final_test.extend(test[1:-1])\n",
    "  \n",
    "  print(evaluate(final_test, final_pred))\n",
    "\n",
    "def load_model(epochs):\n",
    "  path=MODEL_PATH+f\"_{epochs}.bin\"\n",
    "  device = torch.device(DEVICE)\n",
    "  model = EntityModel(num_tag=num_tag)\n",
    "  model.load_state_dict(torch.load(path))\n",
    "  model.to(device)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T11:51:10.191394Z",
     "iopub.status.busy": "2023-12-04T11:51:10.191190Z",
     "iopub.status.idle": "2023-12-04T11:51:18.000887Z",
     "shell.execute_reply": "2023-12-04T11:51:18.000142Z",
     "shell.execute_reply.started": "2023-12-04T11:51:10.191367Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sentences, tag, enc_tag = process_data([data_folder + '/NCBI-disease-IOB/NCBI_train.tsv',\n",
    "                                            data_folder + '/NCBI-disease-IOB/NCBI_dev.tsv'])\n",
    "    test_sentences, test_tag, _ = process_data([data_folder + '/NCBI-disease-IOB/NCBI_test.tsv'])\n",
    "\n",
    "    meta_data = {\n",
    "        \"enc_tag\": enc_tag\n",
    "    }\n",
    "\n",
    "    joblib.dump(meta_data, \"meta.bin\")\n",
    "\n",
    "    num_tag = len(list(enc_tag.classes_))\n",
    "\n",
    "    (\n",
    "        train_sentences,\n",
    "        valid_sentences,\n",
    "        train_tag,\n",
    "        valid_tag\n",
    "    ) = model_selection.train_test_split(sentences, tag, random_state=42, test_size=0.1)\n",
    "\n",
    "    train_dataset = EntityDataset(texts=train_sentences, tags=train_tag,enc_tag=enc_tag)\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=NUM_WORKER)\n",
    "\n",
    "    valid_dataset = EntityDataset(texts=valid_sentences, tags=valid_tag,enc_tag=enc_tag)\n",
    "\n",
    "    valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=VALID_BATCH_SIZE, num_workers=NUM_WORKER)\n",
    "\n",
    "    test_dataset=EntityDataset(texts=test_sentences,tags=test_tag,enc_tag=enc_tag)\n",
    "\n",
    "    device = torch.device(DEVICE)\n",
    "    model = EntityModel(num_tag=num_tag)\n",
    "    model.to(device)\n",
    "\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=3e-5, weight_decay=0.001)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T11:51:18.003951Z",
     "iopub.status.busy": "2023-12-04T11:51:18.003244Z",
     "iopub.status.idle": "2023-12-04T11:51:34.217123Z",
     "shell.execute_reply": "2023-12-04T11:51:34.215940Z",
     "shell.execute_reply.started": "2023-12-04T11:51:18.003906Z"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_fn(train_data_loader, model, optimizer, device)\n",
    "    torch.cuda.empty_cache()\n",
    "    valid_loss = eval_fn(valid_data_loader, model, device)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Train Loss = {train_loss}\")\n",
    "    print(f\"Validation Loss = {valid_loss}\")\n",
    "    test_fn(test_dataset, model, device,enc_tag)\n",
    "    torch.save(model.state_dict(), MODEL_PATH+f\"_{epoch}.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-04T11:51:34.218298Z",
     "iopub.status.idle": "2023-12-04T11:51:34.218749Z",
     "shell.execute_reply": "2023-12-04T11:51:34.218533Z",
     "shell.execute_reply.started": "2023-12-04T11:51:34.218508Z"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import torch\n",
    "import argparse\n",
    "\n",
    "def predict_sentence(model, sentence, enc_tag):\n",
    "    sentence = sentence.split()\n",
    "    test_dataset = EntityDataset(\n",
    "        texts=[sentence], \n",
    "        # pos=[[0] * len(sentence)], \n",
    "        tags=[[0] * len(sentence)],\n",
    "        enc_tag=enc_tag\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        data = test_dataset[0]\n",
    "        for k, v in data.items():\n",
    "            data[k] = v.to(device).unsqueeze(0)\n",
    "\n",
    "        tag = model.encode(**data)\n",
    "        tag = enc_tag.inverse_transform(tag[0])\n",
    "        # pos = enc_pos.inverse_transform(pos[0])\n",
    "\n",
    "    return tag\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sentence = \"Management of Critically Ill Patients with Severe Acute Respiratory Syndrome (SARS).\"\n",
    "\n",
    "    # join the arr -> string sentence (nargs='+', dont have to use \"\" when enter the string)\n",
    "    tokenized_sentence = TOKENIZER.encode(sentence)\n",
    "    tokenized = TOKENIZER.tokenize(sentence)\n",
    "\n",
    "    # meta_data: enc_pos/enc_tag - POS/TAG label encoder\n",
    "    meta_data = joblib.load(\"meta.bin\")\n",
    "    # enc_pos = meta_data[\"enc_pos\"]\n",
    "    enc_tag = meta_data[\"enc_tag\"]\n",
    "\n",
    "    # num_pos = len(list(enc_pos.classes_))\n",
    "    num_tag = len(list(enc_tag.classes_))\n",
    "\n",
    "    # set up device, model\n",
    "    device = torch.device(DEVICE)\n",
    "    model = EntityModel(num_tag=num_tag)\n",
    "    model.load_state_dict(torch.load(MODEL_PATH+f\"_{9}.bin\"))\n",
    "    model.to(device)\n",
    "\n",
    "    tags = predict_sentence(model, sentence, enc_tag)\n",
    "\n",
    "    print(sentence)\n",
    "    print(len(tags),len(tokenized_sentence))\n",
    "    for token,tag in zip(tokenized_sentence,tags):\n",
    "      print(token,TOKENIZER.decode(token),tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1997938,
     "sourceId": 3355427,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2022632,
     "sourceId": 3355435,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30179,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
