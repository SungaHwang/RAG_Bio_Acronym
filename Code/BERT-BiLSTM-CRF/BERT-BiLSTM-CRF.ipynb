{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f4d52fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torchcrf import CRF\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4564b4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/media/lurker18/Local Disk/BioNER-Abbrev/Dataset/NCBI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b3f3216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-05 14:01:51,092] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./pretrained_model/pubmedbert/tokenizer_config.json',\n",
       " './pretrained_model/pubmedbert/special_tokens_map.json',\n",
       " './pretrained_model/pubmedbert/vocab.txt',\n",
       " './pretrained_model/pubmedbert/added_tokens.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\"\n",
    "\n",
    "pubmedbert = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast = False)\n",
    "pubmedbert.save_pretrained(f\"./pretrained_model/pubmedbert\")\n",
    "tokenizer.save_pretrained(\"./pretrained_model/pubmedbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2505cb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Settings\n",
    "device = \"cuda:0\"\n",
    "max_len = 256\n",
    "train_batch_size = 64\n",
    "valid_batch_size = 64\n",
    "epochs = 30\n",
    "num_workers = 8\n",
    "BASE_MODEL_PATH = './pretrained_model/pubmedbert'\n",
    "model_path = \"models/NBCI_pubmedbert\"\n",
    "TOKENIZER = transformers.BertTokenizer.from_pretrained(BASE_MODEL_PATH,\n",
    "                                                       do_lower_case = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0f60177",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityDataset:\n",
    "    def __init__(self, texts, tags,enc_tag):\n",
    "        self.texts = texts\n",
    "        self.tags = tags\n",
    "        self.enc_tag = enc_tag\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item]\n",
    "        tags = self.tags[item]\n",
    "\n",
    "        ids = []\n",
    "        target_tag =[]\n",
    "\n",
    "        for i, s in enumerate(text):\n",
    "            inputs = TOKENIZER.encode(\n",
    "                str(s),\n",
    "                add_special_tokens = False\n",
    "            )\n",
    "            input_len = len(inputs)\n",
    "            ids.extend(inputs)\n",
    "            target_tag.extend([tags[i]] * input_len)\n",
    "\n",
    "        ids = ids[:max_len - 2]\n",
    "        target_tag = target_tag[:max_len - 2]\n",
    "\n",
    "        ids = [102] + ids + [103]\n",
    "        o_tag = self.enc_tag.transform([\"O\"])[0]\n",
    "        target_tag = [o_tag] + target_tag + [o_tag]\n",
    "\n",
    "        mask = [1] * len(ids)\n",
    "        token_type_ids = [0] * len(ids)\n",
    "\n",
    "        padding_len = max_len - len(ids)\n",
    "\n",
    "        ids = ids + ([0] * padding_len)\n",
    "        mask = mask + ([0] * padding_len)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
    "        target_tag = target_tag + ([0] * padding_len)\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(ids, dtype = torch.long),\n",
    "            \"mask\": torch.tensor(mask, dtype = torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            \"target_tag\": torch.tensor(target_tag, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acddd788",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EntityModel(nn.Module):\n",
    "    def __init__(self, num_tag):\n",
    "        super(EntityModel, self).__init__()\n",
    "        self.num_tag = num_tag\n",
    "        self.bert = transformers.BertModel.from_pretrained(BASE_MODEL_PATH, return_dict = False)\n",
    "        self.bilstm =  nn.LSTM(768, 1024 // 2, num_layers = 1, bidirectional = True, batch_first = True)\n",
    "\n",
    "        self.dropout_tag = nn.Dropout(0.3)\n",
    "        \n",
    "        self.hidden2tag_tag = nn.Linear(1024, self.num_tag)\n",
    "\n",
    "        self.crf_tag = CRF(self.num_tag, batch_first = True)\n",
    "    \n",
    "    \n",
    "    # return the loss only, not encode the tag\n",
    "    def forward(self, ids, mask, token_type_ids, target_tag):\n",
    "        x, _ = self.bert(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
    "        h, _ = self.bilstm(x)\n",
    "\n",
    "        o_tag = self.dropout_tag(h)\n",
    "        tag = self.hidden2tag_tag(o_tag)\n",
    "        mask = torch.where(mask == 1, True, False)\n",
    "\n",
    "        loss_tag = - self.crf_tag(tag, \n",
    "                                  target_tag, \n",
    "                                  mask = mask, \n",
    "                                  reduction = 'token_mean')\n",
    "        loss = loss_tag\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "    # encode the tag, dont return loss\n",
    "    def encode(self, ids, mask, token_type_ids, target_tag):\n",
    "        # Bert - BiLSTM\n",
    "        x, _ = self.bert(ids, \n",
    "                         attention_mask = mask, \n",
    "                         token_type_ids = token_type_ids)\n",
    "        h, _ = self.bilstm(x)\n",
    "\n",
    "        # drop out\n",
    "        o_tag = self.dropout_tag(h)\n",
    "        # o_pos = self.dropout_pos(h)\n",
    "\n",
    "        # Hidden2Tag (Linear)\n",
    "        tag = self.hidden2tag_tag(o_tag)\n",
    "        mask = torch.where(mask == 1, True, False)\n",
    "        tag = self.crf_tag.decode(tag, mask = mask)\n",
    "\n",
    "        return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a2d4c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tags = []\n",
    "with open(data_folder + \"/classes.txt\") as f:\n",
    "    for line in f.readlines():\n",
    "        total_tags.append(line.strip())\n",
    "        \n",
    "\n",
    "enc_tag = preprocessing.LabelEncoder()\n",
    "enc_tag.fit(list(total_tags))\n",
    "\n",
    "def process_data(data_path):\n",
    "    sentences, tags = [], []\n",
    "    sentence, tag = [], []\n",
    "    \n",
    "    total_tags = set()\n",
    "    i = 0\n",
    "    \n",
    "    for path in data_path:\n",
    "        with open(path, \"r\") as f:\n",
    "            for line in f:\n",
    "                if i % 10000 == 0:\n",
    "                    print(len(sentences))\n",
    "                \n",
    "                i += 1\n",
    "                line = line.strip()\n",
    "                if line.startswith(\"-DOCSTART-\"):\n",
    "                    continue\n",
    "                elif len(line) == 0:\n",
    "                    if sentence == [] and tag == []:\n",
    "                        continue\n",
    "                        \n",
    "                    sentences.append(sentence)\n",
    "                    tags.append(tag)\n",
    "                    sentence, tag = [], []\n",
    "                else:\n",
    "                    s,t = line.split(\"\\t\")\n",
    "                    sentence.append(s)\n",
    "                    tag.append(t)\n",
    "                    \n",
    "    for i in range(len(tags)):\n",
    "        tags[i] = enc_tag.transform(tags[i])\n",
    "        \n",
    "    return sentences, tags, enc_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34b9f9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotation Tagging \n",
    "\n",
    "def split_tag(chunk_tag):\n",
    "    \"\"\"\n",
    "    split chunk tag into IOBES prefix and chunk_type\n",
    "    e.g.\n",
    "    B-PER --> (B, PER)\n",
    "    O --> (0, None)\n",
    "    \"\"\"\n",
    "    if chunk_tag == 'O':\n",
    "        return ('O', None)\n",
    "    return chunk_tag.split(\"-\", maxsplit = 1)\n",
    "\n",
    "def is_chunk_end(prev_tag, tag):\n",
    "    \"\"\"\n",
    "    check if the previous chunk ended between the previous and current word\n",
    "    e.g.\n",
    "    (B-PER, I-PER) --> False\n",
    "    (B-LOC, O) --> True\n",
    "    Note: in case of contradicting tags, e.g. (B-PER, I-LOC)\n",
    "    this is considered as (B-PER, B-LOC)\n",
    "    \"\"\"\n",
    "    \n",
    "    prefix1, chunk_type1 = split_tag(prev_tag)\n",
    "    prefix2, chunk_type2 = split_tag(tag)\n",
    "    \n",
    "    if prefix1 == 'O':\n",
    "        return False\n",
    "    if prefix2 == 'O':\n",
    "        return prefix1 != 'O'\n",
    "    \n",
    "    if chunk_type1 != chunk_type2:\n",
    "        return True\n",
    "    \n",
    "    return prefix2 in [\"B\", \"S\"] or prefix1 in [\"E\", \"S\"]\n",
    "\n",
    "def is_chunk_start(prev_tag, tag):\n",
    "    \"\"\"\n",
    "    check if a new chunk started between the previous and current word\n",
    "    \"\"\"\n",
    "    prefix1, chunk_type1 = split_tag(prev_tag)\n",
    "    prefix2, chunk_type2 = split_tag(tag)\n",
    "    \n",
    "    if prefix2 == \"O\":\n",
    "        return False\n",
    "    if prefix1 == \"O\":\n",
    "        return prefix2 != \"O\"\n",
    "    \n",
    "    if chunk_type1 != chunk_type2:\n",
    "        return True\n",
    "    \n",
    "    return prefix2 in [\"B\", \"S\"] or prefix1 in [\"E\", \"S\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4203c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(tp, p, t, percent = True):\n",
    "    \"\"\"\n",
    "    compute overall precision, recall and F1-Score (default values are 0.0)\n",
    "    if percent is True, return 100 * original decimal value\n",
    "    \"\"\"\n",
    "    precision = tp / p if p else 0\n",
    "    recall = tp / t if t else 0\n",
    "    fb1 = 2 * precision * recall / (precision + recall) if precision + recall else 0\n",
    "    if percent:\n",
    "        return 100 * precision, 100 * recall, 100 * fb1\n",
    "    else:\n",
    "        return precision, recall, fb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3cdd6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_chunks(true_seqs, pred_seqs):\n",
    "    \n",
    "    \"\"\"\n",
    "    true_seqs: a list of true tags\n",
    "    pred_seqs: a list of predicted tags\n",
    "    return: \n",
    "    correct_chunks: a dict (counter), \n",
    "                    key = chunk types, \n",
    "                    value = number of correctly identified chunks per type\n",
    "    true_chunks:    a dict, number of true chunks per type\n",
    "    pred_chunks:    a dict, number of identified chunks per type\n",
    "    correct_counts, true_counts, pred_counts: similar to above, but for tags\n",
    "    \"\"\"\n",
    "    \n",
    "    correct_chunks = defaultdict(int)\n",
    "    true_chunks = defaultdict(int)\n",
    "    pred_chunks = defaultdict(int)\n",
    "\n",
    "    correct_counts = defaultdict(int)\n",
    "    true_counts = defaultdict(int)\n",
    "    pred_counts = defaultdict(int)\n",
    "\n",
    "    prev_true_tag, prev_pred_tag = 'O', 'O'\n",
    "    correct_chunk = None\n",
    "\n",
    "    for true_tag, pred_tag in zip(true_seqs, pred_seqs):\n",
    "        if true_tag == pred_tag:\n",
    "            correct_counts[true_tag] += 1\n",
    "        true_counts[true_tag] += 1\n",
    "        pred_counts[pred_tag] += 1\n",
    "\n",
    "        _, true_type = split_tag(true_tag)\n",
    "        _, pred_type = split_tag(pred_tag)\n",
    "\n",
    "        if correct_chunk is not None:\n",
    "            true_end = is_chunk_end(prev_true_tag, true_tag)\n",
    "            pred_end = is_chunk_end(prev_pred_tag, pred_tag)\n",
    "\n",
    "            if pred_end and true_end:\n",
    "                correct_chunks[correct_chunk] += 1\n",
    "                correct_chunk = None\n",
    "            elif pred_end != true_end or true_type != pred_type:\n",
    "                correct_chunk = None\n",
    "\n",
    "        true_start = is_chunk_start(prev_true_tag, true_tag)\n",
    "        pred_start = is_chunk_start(prev_pred_tag, pred_tag)\n",
    "\n",
    "        if true_start and pred_start and true_type == pred_type:\n",
    "            correct_chunk = true_type\n",
    "        if true_start:\n",
    "            true_chunks[true_type] += 1\n",
    "        if pred_start:\n",
    "            pred_chunks[pred_type] += 1\n",
    "\n",
    "        prev_true_tag, prev_pred_tag = true_tag, pred_tag\n",
    "    if correct_chunk is not None:\n",
    "        correct_chunks[correct_chunk] += 1\n",
    "\n",
    "    return (correct_chunks, true_chunks, pred_chunks, \n",
    "        correct_counts, true_counts, pred_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93ca0c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "def get_result(correct_chunks, true_chunks, pred_chunks,\n",
    "    correct_counts, true_counts, pred_counts, verbose = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    if verbose, print overall performance, as well as preformance per chunk type;\n",
    "    otherwise, simply return overall prec, rec, f1 scores\n",
    "    \"\"\"\n",
    "    \n",
    "    # sum counts\n",
    "    sum_correct_chunks = sum(correct_chunks.values())\n",
    "    sum_true_chunks = sum(true_chunks.values())\n",
    "    sum_pred_chunks = sum(pred_chunks.values())\n",
    "\n",
    "    sum_correct_counts = sum(correct_counts.values())\n",
    "    sum_true_counts = sum(true_counts.values())\n",
    "\n",
    "    nonO_correct_counts = sum(v for k, v in correct_counts.items() if k != 'O')\n",
    "    nonO_true_counts = sum(v for k, v in true_counts.items() if k != 'O')\n",
    "\n",
    "    chunk_types = sorted(list(set(list(true_chunks) + list(pred_chunks))))\n",
    "\n",
    "    # compute overall precision, recall and F1-Score (default values are 0.0)\n",
    "    prec, rec, f1 = calc_metrics(sum_correct_chunks, sum_pred_chunks, sum_true_chunks)\n",
    "    res = (prec, rec, f1)\n",
    "    if not verbose:\n",
    "        return res\n",
    "\n",
    "    # print overall performance, and performance per chunk type\n",
    "    \n",
    "    print(\"processed %i tokens with %i phrases; \" % (sum_true_counts, sum_true_chunks), end='')\n",
    "    print(\"found: %i phrases; correct: %i.\\n\" % (sum_pred_chunks, sum_correct_chunks), end='')\n",
    "        \n",
    "    print(\"accuracy: %6.2f%%; (non-O)\" % (100 * nonO_correct_counts/nonO_true_counts))\n",
    "    print(\"accuracy: %6.2f%%; \" % (100 * sum_correct_counts/sum_true_counts), end = '')\n",
    "    print(\"precision: %6.2f%%; recall: %6.2f%%; F1-Score: %6.2f\" % (prec, rec, f1))\n",
    "\n",
    "    # for each chunk type, compute precision, recall and FB1 (default values are 0.0)\n",
    "    for t in chunk_types:\n",
    "        prec, rec, f1 = calc_metrics(correct_chunks[t], pred_chunks[t], true_chunks[t])\n",
    "        print(\"%17s: \" %t , end = '')\n",
    "        print(\"precision: %6.2f%%; recall: %6.2f%%; F1-Score: %6.2f\" % (prec, rec, f1), end = '')\n",
    "        print(\"  %d\" % pred_chunks[t])\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "518ae685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the datasets\n",
    "def evaluate(true_seqs, pred_seqs, verbose = True):\n",
    "    (correct_chunks, true_chunks, pred_chunks, correct_counts, true_counts, pred_counts) = count_chunks(true_seqs, pred_seqs)\n",
    "    result = get_result(correct_chunks, true_chunks, pred_chunks, correct_counts, true_counts, pred_counts, verbose = verbose)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b60ef38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the train / validation / test set for each loss\n",
    "def train_fn(data_loader, model, optimizer, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    for data in tqdm(data_loader, total = len(data_loader)):\n",
    "        for k, v in data.items():\n",
    "            data[k] = v.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(**data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        final_loss += loss.item()\n",
    "    return final_loss / len(data_loader)\n",
    "\n",
    "def eval_fn(data_loader, model, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for data in tqdm(data_loader, total = len(data_loader)):\n",
    "        for k, v in data.items():\n",
    "            data[k] = v.to(device)\n",
    "        loss = model(**data)\n",
    "        final_loss += loss.item()\n",
    "    return final_loss / len(data_loader)\n",
    "\n",
    "\n",
    "def test_fn(dataset,model,device,enc_tag):\n",
    "    final_test = []\n",
    "    final_pred = []\n",
    "    O = enc_tag.transform([\"O\"])[0]\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataset):\n",
    "            for k, v in data.items():\n",
    "                data[k] = v.to(device).unsqueeze(0)\n",
    "\n",
    "            tag = model.encode(**data)\n",
    "            padded_pred = tag[0]\n",
    "            test = data[\"target_tag\"].cpu()[0][:len(padded_pred)]\n",
    "            test = enc_tag.inverse_transform(test)\n",
    "            padded_pred = enc_tag.inverse_transform(padded_pred)\n",
    "            final_pred.extend(padded_pred[1:-1])\n",
    "            final_test.extend(test[1:-1])\n",
    "  \n",
    "    print(evaluate(final_test, final_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15c5571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "def load_model(epochs):\n",
    "    path = model_path + f\"_{epochs}.bin\"\n",
    "    device = torch.device(device)\n",
    "    model = EntityModel(num_tag = num_tag)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bef385ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "385\n",
      "766\n",
      "1146\n",
      "1532\n",
      "1907\n",
      "2300\n",
      "2690\n",
      "3083\n",
      "3459\n",
      "3839\n",
      "4249\n",
      "4631\n",
      "5007\n",
      "5383\n",
      "5766\n",
      "6126\n",
      "0\n",
      "369\n",
      "734\n"
     ]
    }
   ],
   "source": [
    "# Load the NCBI Disease\n",
    "if __name__ == \"__main__\":\n",
    "    sentences, tag, enc_tag = process_data([data_folder + '/NCBI-disease-IOB/train.tsv',\n",
    "                                            data_folder + '/NCBI-disease-IOB/dev.tsv'])\n",
    "    test_sentences, test_tag, _ = process_data([data_folder + '/NCBI-disease-IOB/test.tsv'])\n",
    "\n",
    "    meta_data = {\n",
    "        \"enc_tag\": enc_tag\n",
    "    }\n",
    "\n",
    "    joblib.dump(meta_data, \"meta.bin\")\n",
    "\n",
    "    num_tag = len(list(enc_tag.classes_))\n",
    "\n",
    "    (\n",
    "        train_sentences,\n",
    "        valid_sentences,\n",
    "        train_tag,\n",
    "        valid_tag\n",
    "    ) = model_selection.train_test_split(sentences, \n",
    "                                         tag, \n",
    "                                         random_state = 42, \n",
    "                                         test_size = 0.1)\n",
    "\n",
    "    train_dataset = EntityDataset(texts = train_sentences, \n",
    "                                  tags = train_tag,\n",
    "                                  enc_tag = enc_tag)\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                                    batch_size = train_batch_size, \n",
    "                                                    num_workers = num_workers)\n",
    "\n",
    "    valid_dataset = EntityDataset(texts = valid_sentences, \n",
    "                                  tags = valid_tag,\n",
    "                                  enc_tag = enc_tag)\n",
    "\n",
    "    valid_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
    "                                                    batch_size = valid_batch_size, \n",
    "                                                    num_workers = num_workers)\n",
    "\n",
    "    test_dataset = EntityDataset(texts = test_sentences,\n",
    "                                 tags = test_tag, \n",
    "                                 enc_tag = enc_tag)\n",
    "\n",
    "    device = torch.device(device)\n",
    "    model = EntityModel(num_tag = num_tag)\n",
    "    model.to(device)\n",
    "\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=3e-5, weight_decay=0.001)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1babe96a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:05<00:00,  1.37it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:02<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.19001987568206258\n",
      "Validation loss = 0.10474997013807297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:14<00:00, 64.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1376 phrases; correct: 624.\n",
      "accuracy:  74.32%; (non-O)\n",
      "accuracy:  95.64%; precision:  45.35%; recall:  45.85%; F1-Score:  45.60\n",
      "          Disease: precision:  45.35%; recall:  45.85%; F1-Score:  45.60  1376\n",
      "(45.348837209302324, 45.8486407053637, 45.59736938253562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:06<00:00,  1.34it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:03<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.09815076076322132\n",
      "Validation loss = 0.07882284633815288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:14<00:00, 64.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1455 phrases; correct: 737.\n",
      "accuracy:  77.46%; (non-O)\n",
      "accuracy:  96.32%; precision:  50.65%; recall:  54.15%; F1-Score:  52.34\n",
      "          Disease: precision:  50.65%; recall:  54.15%; F1-Score:  52.34  1455\n",
      "(50.65292096219931, 54.15135929463629, 52.34375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:05<00:00,  1.37it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:03<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.06868906937953499\n",
      "Validation loss = 0.06027600765228271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:15<00:00, 61.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1375 phrases; correct: 1030.\n",
      "accuracy:  86.29%; (non-O)\n",
      "accuracy:  97.53%; precision:  74.91%; recall:  75.68%; F1-Score:  75.29\n",
      "          Disease: precision:  74.91%; recall:  75.68%; F1-Score:  75.29  1375\n",
      "(74.90909090909092, 75.67964731814843, 75.29239766081874)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:07<00:00,  1.33it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:02<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.055130862527423434\n",
      "Validation loss = 0.056658771634101865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:15<00:00, 59.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1483 phrases; correct: 1188.\n",
      "accuracy:  90.95%; (non-O)\n",
      "accuracy:  97.84%; precision:  80.11%; recall:  87.29%; F1-Score:  83.54\n",
      "          Disease: precision:  80.11%; recall:  87.29%; F1-Score:  83.54  1483\n",
      "(80.10788941335132, 87.28875826598089, 83.54430379746836)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:06<00:00,  1.36it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:02<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.04319271598425176\n",
      "Validation loss = 0.053891933709383014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:15<00:00, 59.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1455 phrases; correct: 1198.\n",
      "accuracy:  92.42%; (non-O)\n",
      "accuracy:  97.87%; precision:  82.34%; recall:  88.02%; F1-Score:  85.09\n",
      "          Disease: precision:  82.34%; recall:  88.02%; F1-Score:  85.09  1455\n",
      "(82.33676975945016, 88.02351212343865, 85.08522727272727)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:05<00:00,  1.37it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:02<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.03621464369611608\n",
      "Validation loss = 0.04939848203212023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:15<00:00, 60.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1424 phrases; correct: 1192.\n",
      "accuracy:  92.01%; (non-O)\n",
      "accuracy:  97.89%; precision:  83.71%; recall:  87.58%; F1-Score:  85.60\n",
      "          Disease: precision:  83.71%; recall:  87.58%; F1-Score:  85.60  1424\n",
      "(83.70786516853933, 87.582659808964, 85.60143626570915)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:04<00:00,  1.39it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:02<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.028283464825815625\n",
      "Validation loss = 0.05527025479823351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:15<00:00, 58.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1425 phrases; correct: 1201.\n",
      "accuracy:  92.50%; (non-O)\n",
      "accuracy:  97.83%; precision:  84.28%; recall:  88.24%; F1-Score:  86.22\n",
      "          Disease: precision:  84.28%; recall:  88.24%; F1-Score:  86.22  1425\n",
      "(84.28070175438597, 88.24393828067598, 86.2167982770998)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:05<00:00,  1.37it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:03<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.024908279115334154\n",
      "Validation loss = 0.0483676765114069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:15<00:00, 59.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1413 phrases; correct: 1200.\n",
      "accuracy:  92.84%; (non-O)\n",
      "accuracy:  97.94%; precision:  84.93%; recall:  88.17%; F1-Score:  86.52\n",
      "          Disease: precision:  84.93%; recall:  88.17%; F1-Score:  86.52  1413\n",
      "(84.92569002123143, 88.1704628949302, 86.51766402307139)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:05<00:00,  1.38it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:02<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.025238811172958876\n",
      "Validation loss = 0.045263377204537394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:15<00:00, 59.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1427 phrases; correct: 1218.\n",
      "accuracy:  92.46%; (non-O)\n",
      "accuracy:  98.08%; precision:  85.35%; recall:  89.49%; F1-Score:  87.37\n",
      "          Disease: precision:  85.35%; recall:  89.49%; F1-Score:  87.37  1427\n",
      "(85.35388927820603, 89.49301983835414, 87.3744619799139)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:05<00:00,  1.37it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:02<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.02073134052205003\n",
      "Validation loss = 0.04592851623892784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:15<00:00, 60.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1461 phrases; correct: 1212.\n",
      "accuracy:  92.01%; (non-O)\n",
      "accuracy:  98.00%; precision:  82.96%; recall:  89.05%; F1-Score:  85.90\n",
      "          Disease: precision:  82.96%; recall:  89.05%; F1-Score:  85.90  1461\n",
      "(82.95687885010267, 89.0521675238795, 85.89652728561305)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:06<00:00,  1.34it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:03<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.019818164475469125\n",
      "Validation loss = 0.0387733044102788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:16<00:00, 57.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1378 phrases; correct: 1201.\n",
      "accuracy:  90.42%; (non-O)\n",
      "accuracy:  98.13%; precision:  87.16%; recall:  88.24%; F1-Score:  87.70\n",
      "          Disease: precision:  87.16%; recall:  88.24%; F1-Score:  87.70  1378\n",
      "(87.15529753265602, 88.24393828067598, 87.6962395034684)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:06<00:00,  1.35it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:03<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.019562922190460894\n",
      "Validation loss = 0.04130301997065544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:14<00:00, 65.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1377 phrases; correct: 1214.\n",
      "accuracy:  91.33%; (non-O)\n",
      "accuracy:  98.32%; precision:  88.16%; recall:  89.20%; F1-Score:  88.68\n",
      "          Disease: precision:  88.16%; recall:  89.20%; F1-Score:  88.68  1377\n",
      "(88.16267247639796, 89.19911829537105, 88.67786705624543)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:04<00:00,  1.39it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:02<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.015764573723491694\n",
      "Validation loss = 0.04363811798393726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:14<00:00, 64.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1377 phrases; correct: 1200.\n",
      "accuracy:  90.08%; (non-O)\n",
      "accuracy:  98.16%; precision:  87.15%; recall:  88.17%; F1-Score:  87.66\n",
      "          Disease: precision:  87.15%; recall:  88.17%; F1-Score:  87.66  1377\n",
      "(87.14596949891067, 88.1704628949302, 87.65522279035793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:07<00:00,  1.33it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:03<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.014478918859579911\n",
      "Validation loss = 0.04906234480440617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:15<00:00, 58.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1290 phrases; correct: 1153.\n",
      "accuracy:  87.73%; (non-O)\n",
      "accuracy:  98.05%; precision:  89.38%; recall:  84.72%; F1-Score:  86.99\n",
      "          Disease: precision:  89.38%; recall:  84.72%; F1-Score:  86.99  1290\n",
      "(89.37984496124031, 84.71711976487877, 86.98604300264053)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:04<00:00,  1.39it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:03<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.011730210212731941\n",
      "Validation loss = 0.04483432993292809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:14<00:00, 65.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1340 phrases; correct: 1163.\n",
      "accuracy:  89.58%; (non-O)\n",
      "accuracy:  97.94%; precision:  86.79%; recall:  85.45%; F1-Score:  86.12\n",
      "          Disease: precision:  86.79%; recall:  85.45%; F1-Score:  86.12  1340\n",
      "(86.7910447761194, 85.45187362233652, 86.11625323954092)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:05<00:00,  1.38it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:03<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.011675250202339763\n",
      "Validation loss = 0.04280924163758755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:15<00:00, 61.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1304 phrases; correct: 1148.\n",
      "accuracy:  89.39%; (non-O)\n",
      "accuracy:  97.95%; precision:  88.04%; recall:  84.35%; F1-Score:  86.15\n",
      "          Disease: precision:  88.04%; recall:  84.35%; F1-Score:  86.15  1304\n",
      "(88.03680981595092, 84.34974283614989, 86.15384615384616)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:07<00:00,  1.33it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:02<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.010639642014737345\n",
      "Validation loss = 0.041527945920825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:14<00:00, 66.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1311 phrases; correct: 1169.\n",
      "accuracy:  88.64%; (non-O)\n",
      "accuracy:  98.04%; precision:  89.17%; recall:  85.89%; F1-Score:  87.50\n",
      "          Disease: precision:  89.17%; recall:  85.89%; F1-Score:  87.50  1311\n",
      "(89.16857360793287, 85.89272593681116, 87.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:03<00:00,  1.41it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:03<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.010424764916792305\n",
      "Validation loss = 0.04567869510501623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:15<00:00, 59.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1327 phrases; correct: 1187.\n",
      "accuracy:  90.11%; (non-O)\n",
      "accuracy:  98.22%; precision:  89.45%; recall:  87.22%; F1-Score:  88.32\n",
      "          Disease: precision:  89.45%; recall:  87.22%; F1-Score:  88.32  1327\n",
      "(89.4498869630746, 87.21528288023512, 88.31845238095238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:05<00:00,  1.38it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:02<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.008065022650407627\n",
      "Validation loss = 0.04661700390279293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:15<00:00, 59.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1364 phrases; correct: 1205.\n",
      "accuracy:  91.89%; (non-O)\n",
      "accuracy:  98.21%; precision:  88.34%; recall:  88.54%; F1-Score:  88.44\n",
      "          Disease: precision:  88.34%; recall:  88.54%; F1-Score:  88.44  1364\n",
      "(88.34310850439883, 88.53783982365907, 88.44036697247707)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:05<00:00,  1.37it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:02<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.008772147884075012\n",
      "Validation loss = 0.044757818710058926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:15<00:00, 59.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1335 phrases; correct: 1181.\n",
      "accuracy:  88.07%; (non-O)\n",
      "accuracy:  98.08%; precision:  88.46%; recall:  86.77%; F1-Score:  87.61\n",
      "          Disease: precision:  88.46%; recall:  86.77%; F1-Score:  87.61  1335\n",
      "(88.46441947565543, 86.77443056576047, 87.6112759643917)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:05<00:00,  1.38it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:02<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.008105124440044164\n",
      "Validation loss = 0.05043441727757454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:15<00:00, 60.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1381 phrases; correct: 1198.\n",
      "accuracy:  92.12%; (non-O)\n",
      "accuracy:  98.09%; precision:  86.75%; recall:  88.02%; F1-Score:  87.38\n",
      "          Disease: precision:  86.75%; recall:  88.02%; F1-Score:  87.38  1381\n",
      "(86.74873280231716, 88.02351212343865, 87.38147337709701)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:05<00:00,  1.37it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:02<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.008563163820266102\n",
      "Validation loss = 0.057709643989801405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:15<00:00, 58.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1402 phrases; correct: 1216.\n",
      "accuracy:  93.71%; (non-O)\n",
      "accuracy:  98.11%; precision:  86.73%; recall:  89.35%; F1-Score:  88.02\n",
      "          Disease: precision:  86.73%; recall:  89.35%; F1-Score:  88.02  1402\n",
      "(86.73323823109843, 89.3460690668626, 88.02026782482807)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:05<00:00,  1.38it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:03<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.006941575434757397\n",
      "Validation loss = 0.053111820481717587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:16<00:00, 57.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1340 phrases; correct: 1190.\n",
      "accuracy:  91.02%; (non-O)\n",
      "accuracy:  98.10%; precision:  88.81%; recall:  87.44%; F1-Score:  88.12\n",
      "          Disease: precision:  88.81%; recall:  87.44%; F1-Score:  88.12  1340\n",
      "(88.80597014925374, 87.43570903747244, 88.11551277304702)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:05<00:00,  1.38it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:02<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.007455688589511233\n",
      "Validation loss = 0.04933057855814695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:14<00:00, 66.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1338 phrases; correct: 1185.\n",
      "accuracy:  91.06%; (non-O)\n",
      "accuracy:  98.18%; precision:  88.57%; recall:  87.07%; F1-Score:  87.81\n",
      "          Disease: precision:  88.57%; recall:  87.07%; F1-Score:  87.81  1338\n",
      "(88.56502242152466, 87.06833210874358, 87.8103001111523)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:05<00:00,  1.37it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:02<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.00534852343520874\n",
      "Validation loss = 0.06376697048544884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:14<00:00, 66.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1413 phrases; correct: 1223.\n",
      "accuracy:  93.52%; (non-O)\n",
      "accuracy:  98.12%; precision:  86.55%; recall:  89.86%; F1-Score:  88.18\n",
      "          Disease: precision:  86.55%; recall:  89.86%; F1-Score:  88.18  1413\n",
      "(86.55343241330502, 89.86039676708303, 88.17591925018024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:09<00:00,  1.29it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:03<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.007158220395083643\n",
      "Validation loss = 0.058331127278506756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:14<00:00, 64.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1404 phrases; correct: 1225.\n",
      "accuracy:  93.71%; (non-O)\n",
      "accuracy:  98.16%; precision:  87.25%; recall:  90.01%; F1-Score:  88.61\n",
      "          Disease: precision:  87.25%; recall:  90.01%; F1-Score:  88.61  1404\n",
      "(87.25071225071225, 90.00734753857458, 88.60759493670886)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:05<00:00,  1.38it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:02<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.004783049996735321\n",
      "Validation loss = 0.06822614520788192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:14<00:00, 65.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1344 phrases; correct: 1159.\n",
      "accuracy:  91.78%; (non-O)\n",
      "accuracy:  97.93%; precision:  86.24%; recall:  85.16%; F1-Score:  85.69\n",
      "          Disease: precision:  86.24%; recall:  85.16%; F1-Score:  85.69  1344\n",
      "(86.23511904761905, 85.15797207935341, 85.6931608133087)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:04<00:00,  1.40it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:02<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.004441009366160466\n",
      "Validation loss = 0.06529605258256196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:14<00:00, 64.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1384 phrases; correct: 1215.\n",
      "accuracy:  93.45%; (non-O)\n",
      "accuracy:  98.22%; precision:  87.79%; recall:  89.27%; F1-Score:  88.52\n",
      "          Disease: precision:  87.79%; recall:  89.27%; F1-Score:  88.52  1384\n",
      "(87.78901734104046, 89.27259368111683, 88.52459016393443)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:06<00:00,  1.35it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:03<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.004440833105602198\n",
      "Validation loss = 0.05652193790301681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:15<00:00, 61.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1408 phrases; correct: 1239.\n",
      "accuracy:  93.37%; (non-O)\n",
      "accuracy:  98.32%; precision:  88.00%; recall:  91.04%; F1-Score:  89.49\n",
      "          Disease: precision:  88.00%; recall:  91.04%; F1-Score:  89.49  1408\n",
      "(87.9971590909091, 91.03600293901543, 89.4907908992416)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [01:04<00:00,  1.40it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:02<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.004520943965892204\n",
      "Validation loss = 0.060908088274300096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 939/939 [00:14<00:00, 64.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 26587 tokens with 1361 phrases; found: 1407 phrases; correct: 1229.\n",
      "accuracy:  93.86%; (non-O)\n",
      "accuracy:  98.21%; precision:  87.35%; recall:  90.30%; F1-Score:  88.80\n",
      "          Disease: precision:  87.35%; recall:  90.30%; F1-Score:  88.80  1407\n",
      "(87.34896943852168, 90.30124908155767, 88.80057803468208)\n"
     ]
    }
   ],
   "source": [
    "# Run the model!\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_fn(train_data_loader, model, optimizer, device)\n",
    "    torch.cuda.empty_cache()\n",
    "    valid_loss = eval_fn(valid_data_loader, model, device)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Train Loss = {train_loss}\")\n",
    "    print(f\"Validation loss = {valid_loss}\")\n",
    "    test_fn(test_dataset, model, device, enc_tag)\n",
    "    torch.save(model.state_dict(), model_path + f\"_{epoch}.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26751590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
