{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 25445,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02,
      "grad_norm": 4.706916809082031,
      "learning_rate": 2.617801047120419e-05,
      "loss": 3.9479,
      "step": 100
    },
    {
      "epoch": 0.04,
      "grad_norm": 4.220341682434082,
      "learning_rate": 5.235602094240838e-05,
      "loss": 1.4765,
      "step": 200
    },
    {
      "epoch": 0.06,
      "grad_norm": 3.817254066467285,
      "learning_rate": 7.853403141361257e-05,
      "loss": 1.3892,
      "step": 300
    },
    {
      "epoch": 0.08,
      "grad_norm": 3.066462993621826,
      "learning_rate": 0.00010471204188481676,
      "loss": 1.4011,
      "step": 400
    },
    {
      "epoch": 0.1,
      "grad_norm": 3.3813130855560303,
      "learning_rate": 0.00013089005235602096,
      "loss": 1.3412,
      "step": 500
    },
    {
      "epoch": 0.12,
      "grad_norm": 3.524448871612549,
      "learning_rate": 0.00015706806282722515,
      "loss": 1.4021,
      "step": 600
    },
    {
      "epoch": 0.14,
      "grad_norm": 4.043799877166748,
      "learning_rate": 0.00018324607329842934,
      "loss": 1.3614,
      "step": 700
    },
    {
      "epoch": 0.16,
      "grad_norm": 5.2425055503845215,
      "learning_rate": 0.00019999895009866668,
      "loss": 1.3577,
      "step": 800
    },
    {
      "epoch": 0.18,
      "grad_norm": 4.628584384918213,
      "learning_rate": 0.000199985016570914,
      "loss": 1.3756,
      "step": 900
    },
    {
      "epoch": 0.2,
      "grad_norm": 4.6494574546813965,
      "learning_rate": 0.00019995488348011233,
      "loss": 1.3506,
      "step": 1000
    },
    {
      "epoch": 0.22,
      "grad_norm": 3.975275754928589,
      "learning_rate": 0.00019990855570842212,
      "loss": 1.4034,
      "step": 1100
    },
    {
      "epoch": 0.24,
      "grad_norm": 3.8677878379821777,
      "learning_rate": 0.00019984604076186467,
      "loss": 1.3444,
      "step": 1200
    },
    {
      "epoch": 0.26,
      "grad_norm": 4.22016716003418,
      "learning_rate": 0.00019976734876910575,
      "loss": 1.3212,
      "step": 1300
    },
    {
      "epoch": 0.28,
      "grad_norm": 3.524113178253174,
      "learning_rate": 0.00019967249247981468,
      "loss": 1.3592,
      "step": 1400
    },
    {
      "epoch": 0.29,
      "grad_norm": 5.238174915313721,
      "learning_rate": 0.0001995614872625986,
      "loss": 1.3446,
      "step": 1500
    },
    {
      "epoch": 0.31,
      "grad_norm": 4.612039566040039,
      "learning_rate": 0.00019943435110251248,
      "loss": 1.3487,
      "step": 1600
    },
    {
      "epoch": 0.33,
      "grad_norm": 4.403815746307373,
      "learning_rate": 0.00019929110459814502,
      "loss": 1.3761,
      "step": 1700
    },
    {
      "epoch": 0.35,
      "grad_norm": 3.955822706222534,
      "learning_rate": 0.00019913177095828158,
      "loss": 1.3641,
      "step": 1800
    },
    {
      "epoch": 0.37,
      "grad_norm": 4.950448989868164,
      "learning_rate": 0.0001989563759981435,
      "loss": 1.3554,
      "step": 1900
    },
    {
      "epoch": 0.39,
      "grad_norm": 5.972597599029541,
      "learning_rate": 0.000198764948135206,
      "loss": 1.3707,
      "step": 2000
    },
    {
      "epoch": 0.41,
      "grad_norm": 5.857226371765137,
      "learning_rate": 0.0001985575183845934,
      "loss": 1.3454,
      "step": 2100
    },
    {
      "epoch": 0.43,
      "grad_norm": 4.223459243774414,
      "learning_rate": 0.00019833412035405462,
      "loss": 1.3794,
      "step": 2200
    },
    {
      "epoch": 0.45,
      "grad_norm": 5.039631366729736,
      "learning_rate": 0.0001980947902385177,
      "loss": 1.3551,
      "step": 2300
    },
    {
      "epoch": 0.47,
      "grad_norm": 4.860080718994141,
      "learning_rate": 0.00019783956681422557,
      "loss": 1.3215,
      "step": 2400
    },
    {
      "epoch": 0.49,
      "grad_norm": 4.999551296234131,
      "learning_rate": 0.00019756849143245364,
      "loss": 1.3445,
      "step": 2500
    },
    {
      "epoch": 0.51,
      "grad_norm": 4.470262050628662,
      "learning_rate": 0.00019728160801280996,
      "loss": 1.35,
      "step": 2600
    },
    {
      "epoch": 0.53,
      "grad_norm": 5.259581089019775,
      "learning_rate": 0.00019697896303611935,
      "loss": 1.3225,
      "step": 2700
    },
    {
      "epoch": 0.55,
      "grad_norm": 4.033473014831543,
      "learning_rate": 0.00019666060553689274,
      "loss": 1.3078,
      "step": 2800
    },
    {
      "epoch": 0.57,
      "grad_norm": 5.204036235809326,
      "learning_rate": 0.00019632658709538238,
      "loss": 1.3193,
      "step": 2900
    },
    {
      "epoch": 0.59,
      "grad_norm": 5.833913326263428,
      "learning_rate": 0.00019597696182922496,
      "loss": 1.2998,
      "step": 3000
    },
    {
      "epoch": 0.61,
      "grad_norm": 4.216305255889893,
      "learning_rate": 0.0001956117863846735,
      "loss": 1.3306,
      "step": 3100
    },
    {
      "epoch": 0.63,
      "grad_norm": 6.411732196807861,
      "learning_rate": 0.00019523111992741945,
      "loss": 1.3328,
      "step": 3200
    },
    {
      "epoch": 0.65,
      "grad_norm": 4.965451717376709,
      "learning_rate": 0.00019483502413300657,
      "loss": 1.3243,
      "step": 3300
    },
    {
      "epoch": 0.67,
      "grad_norm": 5.875300407409668,
      "learning_rate": 0.0001944235631768386,
      "loss": 1.3036,
      "step": 3400
    },
    {
      "epoch": 0.69,
      "grad_norm": 5.3772149085998535,
      "learning_rate": 0.0001939968037237812,
      "loss": 1.3764,
      "step": 3500
    },
    {
      "epoch": 0.71,
      "grad_norm": 6.408710479736328,
      "learning_rate": 0.00019355481491736107,
      "loss": 1.3563,
      "step": 3600
    },
    {
      "epoch": 0.73,
      "grad_norm": 5.78090763092041,
      "learning_rate": 0.00019309766836856341,
      "loss": 1.3142,
      "step": 3700
    },
    {
      "epoch": 0.75,
      "grad_norm": 4.623159408569336,
      "learning_rate": 0.00019262543814422935,
      "loss": 1.3356,
      "step": 3800
    },
    {
      "epoch": 0.77,
      "grad_norm": 5.6300482749938965,
      "learning_rate": 0.00019213820075505574,
      "loss": 1.3511,
      "step": 3900
    },
    {
      "epoch": 0.79,
      "grad_norm": 6.486914157867432,
      "learning_rate": 0.00019163603514319886,
      "loss": 1.326,
      "step": 4000
    },
    {
      "epoch": 0.81,
      "grad_norm": 5.153124809265137,
      "learning_rate": 0.0001911190226694843,
      "loss": 1.3306,
      "step": 4100
    },
    {
      "epoch": 0.83,
      "grad_norm": 5.892248153686523,
      "learning_rate": 0.00019058724710022476,
      "loss": 1.3605,
      "step": 4200
    },
    {
      "epoch": 0.84,
      "grad_norm": 3.7958362102508545,
      "learning_rate": 0.00019004079459364827,
      "loss": 1.3413,
      "step": 4300
    },
    {
      "epoch": 0.86,
      "grad_norm": 5.480286598205566,
      "learning_rate": 0.000189479753685939,
      "loss": 1.3363,
      "step": 4400
    },
    {
      "epoch": 0.88,
      "grad_norm": 5.456240177154541,
      "learning_rate": 0.0001889042152768924,
      "loss": 1.3106,
      "step": 4500
    },
    {
      "epoch": 0.9,
      "grad_norm": 4.443295478820801,
      "learning_rate": 0.00018831427261518775,
      "loss": 1.3585,
      "step": 4600
    },
    {
      "epoch": 0.92,
      "grad_norm": 5.119434356689453,
      "learning_rate": 0.00018771002128328005,
      "loss": 1.3268,
      "step": 4700
    },
    {
      "epoch": 0.94,
      "grad_norm": 4.029204368591309,
      "learning_rate": 0.00018709155918191362,
      "loss": 1.346,
      "step": 4800
    },
    {
      "epoch": 0.96,
      "grad_norm": 5.659585952758789,
      "learning_rate": 0.00018645898651426041,
      "loss": 1.3001,
      "step": 4900
    },
    {
      "epoch": 0.98,
      "grad_norm": 4.539548873901367,
      "learning_rate": 0.00018581240576968496,
      "loss": 1.2951,
      "step": 5000
    },
    {
      "epoch": 1.0,
      "grad_norm": 2.8271167278289795,
      "learning_rate": 0.00018515192170713918,
      "loss": 1.3286,
      "step": 5100
    },
    {
      "epoch": 1.02,
      "grad_norm": 2.844879627227783,
      "learning_rate": 0.00018447764133818916,
      "loss": 1.094,
      "step": 5200
    },
    {
      "epoch": 1.04,
      "grad_norm": 3.0390002727508545,
      "learning_rate": 0.00018378967390967734,
      "loss": 1.1328,
      "step": 5300
    },
    {
      "epoch": 1.06,
      "grad_norm": 3.176027774810791,
      "learning_rate": 0.00018308813088602227,
      "loss": 1.1389,
      "step": 5400
    },
    {
      "epoch": 1.08,
      "grad_norm": 3.6239662170410156,
      "learning_rate": 0.00018237312593115905,
      "loss": 1.1395,
      "step": 5500
    },
    {
      "epoch": 1.1,
      "grad_norm": 3.1850433349609375,
      "learning_rate": 0.0001816447748901238,
      "loss": 1.1577,
      "step": 5600
    },
    {
      "epoch": 1.12,
      "grad_norm": 3.495940923690796,
      "learning_rate": 0.00018090319577028415,
      "loss": 1.1472,
      "step": 5700
    },
    {
      "epoch": 1.14,
      "grad_norm": 3.917081832885742,
      "learning_rate": 0.00018014850872221988,
      "loss": 1.1073,
      "step": 5800
    },
    {
      "epoch": 1.16,
      "grad_norm": 4.209303379058838,
      "learning_rate": 0.00017938083602025601,
      "loss": 1.167,
      "step": 5900
    },
    {
      "epoch": 1.18,
      "grad_norm": 4.7584452629089355,
      "learning_rate": 0.00017860030204265204,
      "loss": 1.1284,
      "step": 6000
    },
    {
      "epoch": 1.2,
      "grad_norm": 3.0883448123931885,
      "learning_rate": 0.00017780703325145014,
      "loss": 1.1335,
      "step": 6100
    },
    {
      "epoch": 1.22,
      "grad_norm": 3.124143362045288,
      "learning_rate": 0.00017700115817198583,
      "loss": 1.1436,
      "step": 6200
    },
    {
      "epoch": 1.24,
      "grad_norm": 3.3414013385772705,
      "learning_rate": 0.0001761828073720642,
      "loss": 1.1747,
      "step": 6300
    },
    {
      "epoch": 1.26,
      "grad_norm": 4.540140628814697,
      "learning_rate": 0.00017535211344080545,
      "loss": 1.1637,
      "step": 6400
    },
    {
      "epoch": 1.28,
      "grad_norm": 3.2339744567871094,
      "learning_rate": 0.00017450921096716279,
      "loss": 1.1724,
      "step": 6500
    },
    {
      "epoch": 1.3,
      "grad_norm": 3.6291046142578125,
      "learning_rate": 0.00017365423651811624,
      "loss": 1.1763,
      "step": 6600
    },
    {
      "epoch": 1.32,
      "grad_norm": 3.326138973236084,
      "learning_rate": 0.00017278732861654626,
      "loss": 1.182,
      "step": 6700
    },
    {
      "epoch": 1.34,
      "grad_norm": 3.4221203327178955,
      "learning_rate": 0.00017190862771879005,
      "loss": 1.1954,
      "step": 6800
    },
    {
      "epoch": 1.36,
      "grad_norm": 2.954575300216675,
      "learning_rate": 0.00017101827619188509,
      "loss": 1.1501,
      "step": 6900
    },
    {
      "epoch": 1.38,
      "grad_norm": 3.8969333171844482,
      "learning_rate": 0.00017011641829050263,
      "loss": 1.2015,
      "step": 7000
    },
    {
      "epoch": 1.4,
      "grad_norm": 2.503614902496338,
      "learning_rate": 0.00016920320013357558,
      "loss": 1.1787,
      "step": 7100
    },
    {
      "epoch": 1.41,
      "grad_norm": 3.7806410789489746,
      "learning_rate": 0.0001682787696806245,
      "loss": 1.1885,
      "step": 7200
    },
    {
      "epoch": 1.43,
      "grad_norm": 3.185995578765869,
      "learning_rate": 0.00016734327670778508,
      "loss": 1.156,
      "step": 7300
    },
    {
      "epoch": 1.45,
      "grad_norm": 2.8614368438720703,
      "learning_rate": 0.0001663968727835414,
      "loss": 1.164,
      "step": 7400
    },
    {
      "epoch": 1.47,
      "grad_norm": 3.442039728164673,
      "learning_rate": 0.00016543971124416892,
      "loss": 1.1878,
      "step": 7500
    },
    {
      "epoch": 1.49,
      "grad_norm": 2.9707512855529785,
      "learning_rate": 0.00016447194716889086,
      "loss": 1.2051,
      "step": 7600
    },
    {
      "epoch": 1.51,
      "grad_norm": 3.3274593353271484,
      "learning_rate": 0.00016349373735475236,
      "loss": 1.1855,
      "step": 7700
    },
    {
      "epoch": 1.53,
      "grad_norm": 3.449704647064209,
      "learning_rate": 0.00016250524029121611,
      "loss": 1.1506,
      "step": 7800
    },
    {
      "epoch": 1.55,
      "grad_norm": 3.519031524658203,
      "learning_rate": 0.00016150661613448413,
      "loss": 1.1523,
      "step": 7900
    },
    {
      "epoch": 1.57,
      "grad_norm": 3.5541305541992188,
      "learning_rate": 0.00016049802668154897,
      "loss": 1.1505,
      "step": 8000
    },
    {
      "epoch": 1.59,
      "grad_norm": 3.3998863697052,
      "learning_rate": 0.0001594796353439797,
      "loss": 1.1752,
      "step": 8100
    },
    {
      "epoch": 1.61,
      "grad_norm": 3.0553059577941895,
      "learning_rate": 0.00015845160712144566,
      "loss": 1.1601,
      "step": 8200
    },
    {
      "epoch": 1.63,
      "grad_norm": 3.5095808506011963,
      "learning_rate": 0.00015741410857498357,
      "loss": 1.211,
      "step": 8300
    },
    {
      "epoch": 1.65,
      "grad_norm": 3.356117010116577,
      "learning_rate": 0.00015636730780001096,
      "loss": 1.1786,
      "step": 8400
    },
    {
      "epoch": 1.67,
      "grad_norm": 3.144204616546631,
      "learning_rate": 0.00015531137439909172,
      "loss": 1.1705,
      "step": 8500
    },
    {
      "epoch": 1.69,
      "grad_norm": 3.241266965866089,
      "learning_rate": 0.00015424647945445685,
      "loss": 1.1692,
      "step": 8600
    },
    {
      "epoch": 1.71,
      "grad_norm": 3.757160186767578,
      "learning_rate": 0.00015317279550028582,
      "loss": 1.1826,
      "step": 8700
    },
    {
      "epoch": 1.73,
      "grad_norm": 3.262904644012451,
      "learning_rate": 0.0001520904964947527,
      "loss": 1.1776,
      "step": 8800
    },
    {
      "epoch": 1.75,
      "grad_norm": 2.920419692993164,
      "learning_rate": 0.00015099975779184126,
      "loss": 1.165,
      "step": 8900
    },
    {
      "epoch": 1.77,
      "grad_norm": 3.4145071506500244,
      "learning_rate": 0.0001499007561129343,
      "loss": 1.1752,
      "step": 9000
    },
    {
      "epoch": 1.79,
      "grad_norm": 4.106423854827881,
      "learning_rate": 0.00014879366951818122,
      "loss": 1.1947,
      "step": 9100
    },
    {
      "epoch": 1.81,
      "grad_norm": 4.032798767089844,
      "learning_rate": 0.00014767867737764876,
      "loss": 1.1901,
      "step": 9200
    },
    {
      "epoch": 1.83,
      "grad_norm": 3.4769158363342285,
      "learning_rate": 0.0001465559603422594,
      "loss": 1.1709,
      "step": 9300
    },
    {
      "epoch": 1.85,
      "grad_norm": 3.8772051334381104,
      "learning_rate": 0.0001454257003145224,
      "loss": 1.1812,
      "step": 9400
    },
    {
      "epoch": 1.87,
      "grad_norm": 3.4170308113098145,
      "learning_rate": 0.00014428808041906188,
      "loss": 1.1977,
      "step": 9500
    },
    {
      "epoch": 1.89,
      "grad_norm": 3.3987789154052734,
      "learning_rate": 0.00014314328497294716,
      "loss": 1.1461,
      "step": 9600
    },
    {
      "epoch": 1.91,
      "grad_norm": 2.8342504501342773,
      "learning_rate": 0.00014199149945582945,
      "loss": 1.1815,
      "step": 9700
    },
    {
      "epoch": 1.93,
      "grad_norm": 3.5295915603637695,
      "learning_rate": 0.00014083291047989072,
      "loss": 1.1792,
      "step": 9800
    },
    {
      "epoch": 1.95,
      "grad_norm": 3.2117812633514404,
      "learning_rate": 0.00013966770575960866,
      "loss": 1.1684,
      "step": 9900
    },
    {
      "epoch": 1.97,
      "grad_norm": 2.628690719604492,
      "learning_rate": 0.00013849607408134327,
      "loss": 1.2197,
      "step": 10000
    },
    {
      "epoch": 1.98,
      "grad_norm": 3.5067551136016846,
      "learning_rate": 0.00013731820527274966,
      "loss": 1.1496,
      "step": 10100
    },
    {
      "epoch": 2.0,
      "grad_norm": 3.580821990966797,
      "learning_rate": 0.00013613429017202223,
      "loss": 1.1321,
      "step": 10200
    },
    {
      "epoch": 2.02,
      "grad_norm": 3.897789478302002,
      "learning_rate": 0.00013494452059697502,
      "loss": 0.9501,
      "step": 10300
    },
    {
      "epoch": 2.04,
      "grad_norm": 3.9657835960388184,
      "learning_rate": 0.00013374908931396352,
      "loss": 0.913,
      "step": 10400
    },
    {
      "epoch": 2.06,
      "grad_norm": 4.012227535247803,
      "learning_rate": 0.00013254819000665257,
      "loss": 0.9238,
      "step": 10500
    },
    {
      "epoch": 2.08,
      "grad_norm": 3.544074296951294,
      "learning_rate": 0.00013134201724463584,
      "loss": 0.9276,
      "step": 10600
    },
    {
      "epoch": 2.1,
      "grad_norm": 3.753830909729004,
      "learning_rate": 0.00013013076645191154,
      "loss": 0.9462,
      "step": 10700
    },
    {
      "epoch": 2.12,
      "grad_norm": 3.3783018589019775,
      "learning_rate": 0.00012891463387522013,
      "loss": 0.9232,
      "step": 10800
    },
    {
      "epoch": 2.14,
      "grad_norm": 4.7551655769348145,
      "learning_rate": 0.000127693816552248,
      "loss": 0.9227,
      "step": 10900
    },
    {
      "epoch": 2.16,
      "grad_norm": 3.8072445392608643,
      "learning_rate": 0.00012646851227970392,
      "loss": 0.9544,
      "step": 11000
    },
    {
      "epoch": 2.18,
      "grad_norm": 3.2444398403167725,
      "learning_rate": 0.00012523891958127168,
      "loss": 0.9591,
      "step": 11100
    },
    {
      "epoch": 2.2,
      "grad_norm": 4.117938995361328,
      "learning_rate": 0.00012400523767544546,
      "loss": 0.9358,
      "step": 11200
    },
    {
      "epoch": 2.22,
      "grad_norm": 4.013585090637207,
      "learning_rate": 0.00012276766644325247,
      "loss": 0.938,
      "step": 11300
    },
    {
      "epoch": 2.24,
      "grad_norm": 3.2444686889648438,
      "learning_rate": 0.00012152640639586823,
      "loss": 0.9594,
      "step": 11400
    },
    {
      "epoch": 2.26,
      "grad_norm": 3.425335645675659,
      "learning_rate": 0.00012028165864212968,
      "loss": 0.9461,
      "step": 11500
    },
    {
      "epoch": 2.28,
      "grad_norm": 4.4263787269592285,
      "learning_rate": 0.00011903362485595165,
      "loss": 0.9466,
      "step": 11600
    },
    {
      "epoch": 2.3,
      "grad_norm": 4.266635417938232,
      "learning_rate": 0.00011778250724365167,
      "loss": 0.9456,
      "step": 11700
    },
    {
      "epoch": 2.32,
      "grad_norm": 3.8482375144958496,
      "learning_rate": 0.00011652850851118842,
      "loss": 0.9474,
      "step": 11800
    },
    {
      "epoch": 2.34,
      "grad_norm": 5.135953426361084,
      "learning_rate": 0.00011527183183131951,
      "loss": 0.9401,
      "step": 11900
    },
    {
      "epoch": 2.36,
      "grad_norm": 3.6947360038757324,
      "learning_rate": 0.00011401268081068324,
      "loss": 0.9577,
      "step": 12000
    },
    {
      "epoch": 2.38,
      "grad_norm": 3.6583313941955566,
      "learning_rate": 0.00011275125945681043,
      "loss": 0.9338,
      "step": 12100
    },
    {
      "epoch": 2.4,
      "grad_norm": 3.353635549545288,
      "learning_rate": 0.00011148777214507098,
      "loss": 0.9532,
      "step": 12200
    },
    {
      "epoch": 2.42,
      "grad_norm": 3.688255786895752,
      "learning_rate": 0.00011022242358556126,
      "loss": 0.9247,
      "step": 12300
    },
    {
      "epoch": 2.44,
      "grad_norm": 3.68198299407959,
      "learning_rate": 0.00010895541878993669,
      "loss": 0.9614,
      "step": 12400
    },
    {
      "epoch": 2.46,
      "grad_norm": 4.047335147857666,
      "learning_rate": 0.00010768696303819593,
      "loss": 0.941,
      "step": 12500
    },
    {
      "epoch": 2.48,
      "grad_norm": 3.699383020401001,
      "learning_rate": 0.00010641726184542147,
      "loss": 0.9432,
      "step": 12600
    },
    {
      "epoch": 2.5,
      "grad_norm": 3.7362945079803467,
      "learning_rate": 0.000105146520928482,
      "loss": 0.9581,
      "step": 12700
    },
    {
      "epoch": 2.52,
      "grad_norm": 3.8508825302124023,
      "learning_rate": 0.00010387494617270224,
      "loss": 0.9526,
      "step": 12800
    },
    {
      "epoch": 2.53,
      "grad_norm": 3.617316484451294,
      "learning_rate": 0.00010260274359850541,
      "loss": 0.924,
      "step": 12900
    },
    {
      "epoch": 2.55,
      "grad_norm": 4.888777732849121,
      "learning_rate": 0.00010133011932803378,
      "loss": 0.948,
      "step": 13000
    },
    {
      "epoch": 2.57,
      "grad_norm": 3.26619029045105,
      "learning_rate": 0.00010005727955175277,
      "loss": 0.9485,
      "step": 13100
    },
    {
      "epoch": 2.59,
      "grad_norm": 2.8669474124908447,
      "learning_rate": 9.878443049504415e-05,
      "loss": 0.9511,
      "step": 13200
    },
    {
      "epoch": 2.61,
      "grad_norm": 3.4197659492492676,
      "learning_rate": 9.751177838479323e-05,
      "loss": 0.951,
      "step": 13300
    },
    {
      "epoch": 2.63,
      "grad_norm": 4.459595203399658,
      "learning_rate": 9.62395294159761e-05,
      "loss": 0.9426,
      "step": 13400
    },
    {
      "epoch": 2.65,
      "grad_norm": 4.078471660614014,
      "learning_rate": 9.496788971825186e-05,
      "loss": 0.9327,
      "step": 13500
    },
    {
      "epoch": 2.67,
      "grad_norm": 3.5208096504211426,
      "learning_rate": 9.369706532256557e-05,
      "loss": 0.9686,
      "step": 13600
    },
    {
      "epoch": 2.69,
      "grad_norm": 4.367499351501465,
      "learning_rate": 9.242726212776725e-05,
      "loss": 0.9358,
      "step": 13700
    },
    {
      "epoch": 2.71,
      "grad_norm": 4.020688056945801,
      "learning_rate": 9.115868586725192e-05,
      "loss": 0.9434,
      "step": 13800
    },
    {
      "epoch": 2.73,
      "grad_norm": 3.6107590198516846,
      "learning_rate": 8.98915420756268e-05,
      "loss": 0.9437,
      "step": 13900
    },
    {
      "epoch": 2.75,
      "grad_norm": 3.0766377449035645,
      "learning_rate": 8.862603605541079e-05,
      "loss": 0.9445,
      "step": 14000
    },
    {
      "epoch": 2.77,
      "grad_norm": 3.763690233230591,
      "learning_rate": 8.736237284377107e-05,
      "loss": 0.9653,
      "step": 14100
    },
    {
      "epoch": 2.79,
      "grad_norm": 4.531647205352783,
      "learning_rate": 8.610075717930318e-05,
      "loss": 0.9358,
      "step": 14200
    },
    {
      "epoch": 2.81,
      "grad_norm": 3.9788951873779297,
      "learning_rate": 8.484139346885932e-05,
      "loss": 0.9422,
      "step": 14300
    },
    {
      "epoch": 2.83,
      "grad_norm": 4.291083812713623,
      "learning_rate": 8.358448575443027e-05,
      "loss": 0.932,
      "step": 14400
    },
    {
      "epoch": 2.85,
      "grad_norm": 2.940433979034424,
      "learning_rate": 8.233023768008652e-05,
      "loss": 0.9437,
      "step": 14500
    },
    {
      "epoch": 2.87,
      "grad_norm": 3.3284971714019775,
      "learning_rate": 8.107885245898402e-05,
      "loss": 0.9416,
      "step": 14600
    },
    {
      "epoch": 2.89,
      "grad_norm": 3.4819250106811523,
      "learning_rate": 7.98305328404394e-05,
      "loss": 0.9125,
      "step": 14700
    },
    {
      "epoch": 2.91,
      "grad_norm": 3.580751895904541,
      "learning_rate": 7.858548107708069e-05,
      "loss": 0.9662,
      "step": 14800
    },
    {
      "epoch": 2.93,
      "grad_norm": 4.014631271362305,
      "learning_rate": 7.73438988920784e-05,
      "loss": 0.9191,
      "step": 14900
    },
    {
      "epoch": 2.95,
      "grad_norm": 3.6028201580047607,
      "learning_rate": 7.610598744646218e-05,
      "loss": 0.9453,
      "step": 15000
    },
    {
      "epoch": 2.97,
      "grad_norm": 2.9539082050323486,
      "learning_rate": 7.487194730652887e-05,
      "loss": 0.9262,
      "step": 15100
    },
    {
      "epoch": 2.99,
      "grad_norm": 3.7792296409606934,
      "learning_rate": 7.364197841134667e-05,
      "loss": 0.9462,
      "step": 15200
    },
    {
      "epoch": 3.01,
      "grad_norm": 3.5976171493530273,
      "learning_rate": 7.241628004036123e-05,
      "loss": 0.8483,
      "step": 15300
    },
    {
      "epoch": 3.03,
      "grad_norm": 4.657468795776367,
      "learning_rate": 7.119505078110805e-05,
      "loss": 0.6483,
      "step": 15400
    },
    {
      "epoch": 3.05,
      "grad_norm": 4.350691795349121,
      "learning_rate": 6.997848849703781e-05,
      "loss": 0.6619,
      "step": 15500
    },
    {
      "epoch": 3.07,
      "grad_norm": 5.227324485778809,
      "learning_rate": 6.87667902954581e-05,
      "loss": 0.6523,
      "step": 15600
    },
    {
      "epoch": 3.09,
      "grad_norm": 4.5781683921813965,
      "learning_rate": 6.756015249559845e-05,
      "loss": 0.6449,
      "step": 15700
    },
    {
      "epoch": 3.1,
      "grad_norm": 5.0186944007873535,
      "learning_rate": 6.635877059680248e-05,
      "loss": 0.6529,
      "step": 15800
    },
    {
      "epoch": 3.12,
      "grad_norm": 3.864598274230957,
      "learning_rate": 6.51628392468533e-05,
      "loss": 0.6682,
      "step": 15900
    },
    {
      "epoch": 3.14,
      "grad_norm": 3.4569766521453857,
      "learning_rate": 6.397255221043649e-05,
      "loss": 0.6361,
      "step": 16000
    },
    {
      "epoch": 3.16,
      "grad_norm": 4.256162166595459,
      "learning_rate": 6.278810233774667e-05,
      "loss": 0.6482,
      "step": 16100
    },
    {
      "epoch": 3.18,
      "grad_norm": 6.664066791534424,
      "learning_rate": 6.160968153324153e-05,
      "loss": 0.6324,
      "step": 16200
    },
    {
      "epoch": 3.2,
      "grad_norm": 4.202533721923828,
      "learning_rate": 6.0437480724549846e-05,
      "loss": 0.6463,
      "step": 16300
    },
    {
      "epoch": 3.22,
      "grad_norm": 5.3717570304870605,
      "learning_rate": 5.927168983153712e-05,
      "loss": 0.643,
      "step": 16400
    },
    {
      "epoch": 3.24,
      "grad_norm": 5.152910232543945,
      "learning_rate": 5.8112497735535034e-05,
      "loss": 0.6419,
      "step": 16500
    },
    {
      "epoch": 3.26,
      "grad_norm": 3.698244094848633,
      "learning_rate": 5.696009224873873e-05,
      "loss": 0.6684,
      "step": 16600
    },
    {
      "epoch": 3.28,
      "grad_norm": 4.302908420562744,
      "learning_rate": 5.581466008377757e-05,
      "loss": 0.6652,
      "step": 16700
    },
    {
      "epoch": 3.3,
      "grad_norm": 3.1147382259368896,
      "learning_rate": 5.467638682346403e-05,
      "loss": 0.6637,
      "step": 16800
    },
    {
      "epoch": 3.32,
      "grad_norm": 6.279616355895996,
      "learning_rate": 5.3545456890725364e-05,
      "loss": 0.6557,
      "step": 16900
    },
    {
      "epoch": 3.34,
      "grad_norm": 5.8663411140441895,
      "learning_rate": 5.242205351872375e-05,
      "loss": 0.6335,
      "step": 17000
    },
    {
      "epoch": 3.36,
      "grad_norm": 4.692448139190674,
      "learning_rate": 5.1306358721168634e-05,
      "loss": 0.6234,
      "step": 17100
    },
    {
      "epoch": 3.38,
      "grad_norm": 3.583763837814331,
      "learning_rate": 5.019855326282683e-05,
      "loss": 0.6438,
      "step": 17200
    },
    {
      "epoch": 3.4,
      "grad_norm": 4.868075847625732,
      "learning_rate": 4.909881663023544e-05,
      "loss": 0.6359,
      "step": 17300
    },
    {
      "epoch": 3.42,
      "grad_norm": 3.8719301223754883,
      "learning_rate": 4.800732700262085e-05,
      "loss": 0.6259,
      "step": 17400
    },
    {
      "epoch": 3.44,
      "grad_norm": 2.8571767807006836,
      "learning_rate": 4.6924261223030654e-05,
      "loss": 0.6265,
      "step": 17500
    },
    {
      "epoch": 3.46,
      "grad_norm": 4.113993167877197,
      "learning_rate": 4.584979476968139e-05,
      "loss": 0.6397,
      "step": 17600
    },
    {
      "epoch": 3.48,
      "grad_norm": 3.31976056098938,
      "learning_rate": 4.4784101727527406e-05,
      "loss": 0.6235,
      "step": 17700
    },
    {
      "epoch": 3.5,
      "grad_norm": 5.108867168426514,
      "learning_rate": 4.372735476005597e-05,
      "loss": 0.6489,
      "step": 17800
    },
    {
      "epoch": 3.52,
      "grad_norm": 3.6008331775665283,
      "learning_rate": 4.267972508131197e-05,
      "loss": 0.6347,
      "step": 17900
    },
    {
      "epoch": 3.54,
      "grad_norm": 4.844017505645752,
      "learning_rate": 4.164138242815829e-05,
      "loss": 0.6336,
      "step": 18000
    },
    {
      "epoch": 3.56,
      "grad_norm": 5.255330562591553,
      "learning_rate": 4.0612495032774535e-05,
      "loss": 0.6345,
      "step": 18100
    },
    {
      "epoch": 3.58,
      "grad_norm": 4.280319690704346,
      "learning_rate": 3.959322959540046e-05,
      "loss": 0.6373,
      "step": 18200
    },
    {
      "epoch": 3.6,
      "grad_norm": 3.8369176387786865,
      "learning_rate": 3.8583751257327106e-05,
      "loss": 0.6337,
      "step": 18300
    },
    {
      "epoch": 3.62,
      "grad_norm": 4.173847198486328,
      "learning_rate": 3.758422357414047e-05,
      "loss": 0.6449,
      "step": 18400
    },
    {
      "epoch": 3.64,
      "grad_norm": 3.850327968597412,
      "learning_rate": 3.6594808489222486e-05,
      "loss": 0.6311,
      "step": 18500
    },
    {
      "epoch": 3.65,
      "grad_norm": 3.978646755218506,
      "learning_rate": 3.561566630751285e-05,
      "loss": 0.6486,
      "step": 18600
    },
    {
      "epoch": 3.67,
      "grad_norm": 4.604000091552734,
      "learning_rate": 3.464695566953644e-05,
      "loss": 0.6273,
      "step": 18700
    },
    {
      "epoch": 3.69,
      "grad_norm": 4.719352722167969,
      "learning_rate": 3.3688833525700304e-05,
      "loss": 0.6216,
      "step": 18800
    },
    {
      "epoch": 3.71,
      "grad_norm": 4.4587531089782715,
      "learning_rate": 3.274145511086475e-05,
      "loss": 0.6421,
      "step": 18900
    },
    {
      "epoch": 3.73,
      "grad_norm": 4.13956356048584,
      "learning_rate": 3.1804973919192135e-05,
      "loss": 0.6166,
      "step": 19000
    },
    {
      "epoch": 3.75,
      "grad_norm": 5.248852729797363,
      "learning_rate": 3.0879541679277605e-05,
      "loss": 0.6377,
      "step": 19100
    },
    {
      "epoch": 3.77,
      "grad_norm": 4.889448642730713,
      "learning_rate": 2.9965308329566355e-05,
      "loss": 0.6099,
      "step": 19200
    },
    {
      "epoch": 3.79,
      "grad_norm": 4.602674961090088,
      "learning_rate": 2.9062421994060285e-05,
      "loss": 0.6284,
      "step": 19300
    },
    {
      "epoch": 3.81,
      "grad_norm": 4.190536022186279,
      "learning_rate": 2.817102895831918e-05,
      "loss": 0.6429,
      "step": 19400
    },
    {
      "epoch": 3.83,
      "grad_norm": 4.3561320304870605,
      "learning_rate": 2.7291273645759498e-05,
      "loss": 0.6233,
      "step": 19500
    },
    {
      "epoch": 3.85,
      "grad_norm": 4.8654279708862305,
      "learning_rate": 2.6423298594254774e-05,
      "loss": 0.6284,
      "step": 19600
    },
    {
      "epoch": 3.87,
      "grad_norm": 5.386683940887451,
      "learning_rate": 2.556724443304185e-05,
      "loss": 0.6388,
      "step": 19700
    },
    {
      "epoch": 3.89,
      "grad_norm": 7.116786003112793,
      "learning_rate": 2.472324985993597e-05,
      "loss": 0.6315,
      "step": 19800
    },
    {
      "epoch": 3.91,
      "grad_norm": 3.6739401817321777,
      "learning_rate": 2.3891451618859106e-05,
      "loss": 0.6218,
      "step": 19900
    },
    {
      "epoch": 3.93,
      "grad_norm": 6.0471110343933105,
      "learning_rate": 2.307198447768456e-05,
      "loss": 0.6247,
      "step": 20000
    },
    {
      "epoch": 3.95,
      "grad_norm": 3.392382860183716,
      "learning_rate": 2.2264981206402047e-05,
      "loss": 0.6183,
      "step": 20100
    },
    {
      "epoch": 3.97,
      "grad_norm": 4.873063564300537,
      "learning_rate": 2.147057255560626e-05,
      "loss": 0.6125,
      "step": 20200
    },
    {
      "epoch": 3.99,
      "grad_norm": 4.919355869293213,
      "learning_rate": 2.0688887235312548e-05,
      "loss": 0.6331,
      "step": 20300
    },
    {
      "epoch": 4.01,
      "grad_norm": 3.4676969051361084,
      "learning_rate": 1.992005189410362e-05,
      "loss": 0.5124,
      "step": 20400
    },
    {
      "epoch": 4.03,
      "grad_norm": 2.9422833919525146,
      "learning_rate": 1.9164191098609598e-05,
      "loss": 0.3789,
      "step": 20500
    },
    {
      "epoch": 4.05,
      "grad_norm": 4.1606597900390625,
      "learning_rate": 1.8421427313326046e-05,
      "loss": 0.3747,
      "step": 20600
    },
    {
      "epoch": 4.07,
      "grad_norm": 4.385756969451904,
      "learning_rate": 1.769188088077204e-05,
      "loss": 0.369,
      "step": 20700
    },
    {
      "epoch": 4.09,
      "grad_norm": 6.120683670043945,
      "learning_rate": 1.6975670001992495e-05,
      "loss": 0.3523,
      "step": 20800
    },
    {
      "epoch": 4.11,
      "grad_norm": 5.660375118255615,
      "learning_rate": 1.627291071740712e-05,
      "loss": 0.3925,
      "step": 20900
    },
    {
      "epoch": 4.13,
      "grad_norm": 3.494532346725464,
      "learning_rate": 1.558371688800957e-05,
      "loss": 0.3858,
      "step": 21000
    },
    {
      "epoch": 4.15,
      "grad_norm": 3.326361894607544,
      "learning_rate": 1.4908200176919784e-05,
      "loss": 0.3762,
      "step": 21100
    },
    {
      "epoch": 4.17,
      "grad_norm": 4.802870750427246,
      "learning_rate": 1.4246470031292158e-05,
      "loss": 0.3858,
      "step": 21200
    },
    {
      "epoch": 4.19,
      "grad_norm": 4.918683052062988,
      "learning_rate": 1.3598633664583139e-05,
      "loss": 0.3821,
      "step": 21300
    },
    {
      "epoch": 4.21,
      "grad_norm": 4.790191650390625,
      "learning_rate": 1.2964796039180326e-05,
      "loss": 0.3779,
      "step": 21400
    },
    {
      "epoch": 4.22,
      "grad_norm": 4.098720073699951,
      "learning_rate": 1.2345059849396644e-05,
      "loss": 0.3844,
      "step": 21500
    },
    {
      "epoch": 4.24,
      "grad_norm": 4.026217937469482,
      "learning_rate": 1.1739525504831684e-05,
      "loss": 0.374,
      "step": 21600
    },
    {
      "epoch": 4.26,
      "grad_norm": 4.5172882080078125,
      "learning_rate": 1.1148291114103415e-05,
      "loss": 0.3675,
      "step": 21700
    },
    {
      "epoch": 4.28,
      "grad_norm": 3.637227773666382,
      "learning_rate": 1.0571452468952636e-05,
      "loss": 0.3652,
      "step": 21800
    },
    {
      "epoch": 4.3,
      "grad_norm": 5.3687825202941895,
      "learning_rate": 1.0009103028722733e-05,
      "loss": 0.3804,
      "step": 21900
    },
    {
      "epoch": 4.32,
      "grad_norm": 4.969491004943848,
      "learning_rate": 9.461333905217528e-06,
      "loss": 0.3612,
      "step": 22000
    },
    {
      "epoch": 4.34,
      "grad_norm": 3.2408607006073,
      "learning_rate": 8.928233847939271e-06,
      "loss": 0.38,
      "step": 22100
    },
    {
      "epoch": 4.36,
      "grad_norm": 3.756110429763794,
      "learning_rate": 8.409889229709456e-06,
      "loss": 0.3573,
      "step": 22200
    },
    {
      "epoch": 4.38,
      "grad_norm": 4.986330032348633,
      "learning_rate": 7.906384032674752e-06,
      "loss": 0.3834,
      "step": 22300
    },
    {
      "epoch": 4.4,
      "grad_norm": 4.959155559539795,
      "learning_rate": 7.417799834700134e-06,
      "loss": 0.3766,
      "step": 22400
    },
    {
      "epoch": 4.42,
      "grad_norm": 4.175516605377197,
      "learning_rate": 6.944215796151765e-06,
      "loss": 0.372,
      "step": 22500
    },
    {
      "epoch": 4.44,
      "grad_norm": 4.117072105407715,
      "learning_rate": 6.485708647071331e-06,
      "loss": 0.3868,
      "step": 22600
    },
    {
      "epoch": 4.46,
      "grad_norm": 6.015859603881836,
      "learning_rate": 6.042352674744323e-06,
      "loss": 0.3834,
      "step": 22700
    },
    {
      "epoch": 4.48,
      "grad_norm": 4.848023414611816,
      "learning_rate": 5.614219711663982e-06,
      "loss": 0.3671,
      "step": 22800
    },
    {
      "epoch": 4.5,
      "grad_norm": 4.041208267211914,
      "learning_rate": 5.201379123893013e-06,
      "loss": 0.3711,
      "step": 22900
    },
    {
      "epoch": 4.52,
      "grad_norm": 3.8878254890441895,
      "learning_rate": 4.803897799824908e-06,
      "loss": 0.3838,
      "step": 23000
    },
    {
      "epoch": 4.54,
      "grad_norm": 3.9980173110961914,
      "learning_rate": 4.421840139346656e-06,
      "loss": 0.3644,
      "step": 23100
    },
    {
      "epoch": 4.56,
      "grad_norm": 3.9059383869171143,
      "learning_rate": 4.055268043404758e-06,
      "loss": 0.3701,
      "step": 23200
    },
    {
      "epoch": 4.58,
      "grad_norm": 2.7009353637695312,
      "learning_rate": 3.704240903975953e-06,
      "loss": 0.3655,
      "step": 23300
    },
    {
      "epoch": 4.6,
      "grad_norm": 5.341463565826416,
      "learning_rate": 3.3688155944445964e-06,
      "loss": 0.3675,
      "step": 23400
    },
    {
      "epoch": 4.62,
      "grad_norm": 3.915308952331543,
      "learning_rate": 3.0490464603880274e-06,
      "loss": 0.378,
      "step": 23500
    },
    {
      "epoch": 4.64,
      "grad_norm": 4.851292133331299,
      "learning_rate": 2.7449853107714863e-06,
      "loss": 0.3751,
      "step": 23600
    },
    {
      "epoch": 4.66,
      "grad_norm": 3.1952807903289795,
      "learning_rate": 2.4566814095540514e-06,
      "loss": 0.3717,
      "step": 23700
    },
    {
      "epoch": 4.68,
      "grad_norm": 4.402431488037109,
      "learning_rate": 2.1841814677068583e-06,
      "loss": 0.3825,
      "step": 23800
    },
    {
      "epoch": 4.7,
      "grad_norm": 3.6216397285461426,
      "learning_rate": 1.9275296356450535e-06,
      "loss": 0.3642,
      "step": 23900
    },
    {
      "epoch": 4.72,
      "grad_norm": 6.469264507293701,
      "learning_rate": 1.6867674960745039e-06,
      "loss": 0.348,
      "step": 24000
    },
    {
      "epoch": 4.74,
      "grad_norm": 4.048853397369385,
      "learning_rate": 1.4619340572545882e-06,
      "loss": 0.3712,
      "step": 24100
    },
    {
      "epoch": 4.76,
      "grad_norm": 5.110210418701172,
      "learning_rate": 1.2530657466780615e-06,
      "loss": 0.3752,
      "step": 24200
    },
    {
      "epoch": 4.78,
      "grad_norm": 4.534286022186279,
      "learning_rate": 1.0601964051691116e-06,
      "loss": 0.3654,
      "step": 24300
    },
    {
      "epoch": 4.79,
      "grad_norm": 3.6460001468658447,
      "learning_rate": 8.833572814004321e-07,
      "loss": 0.3723,
      "step": 24400
    },
    {
      "epoch": 4.81,
      "grad_norm": 4.8408522605896,
      "learning_rate": 7.225770268303289e-07,
      "loss": 0.3807,
      "step": 24500
    },
    {
      "epoch": 4.83,
      "grad_norm": 4.179936408996582,
      "learning_rate": 5.778816910606666e-07,
      "loss": 0.3717,
      "step": 24600
    },
    {
      "epoch": 4.85,
      "grad_norm": 4.206439018249512,
      "learning_rate": 4.492947176162221e-07,
      "loss": 0.375,
      "step": 24700
    },
    {
      "epoch": 4.87,
      "grad_norm": 4.1366353034973145,
      "learning_rate": 3.368369401464233e-07,
      "loss": 0.389,
      "step": 24800
    },
    {
      "epoch": 4.89,
      "grad_norm": 4.176536560058594,
      "learning_rate": 2.405265790498823e-07,
      "loss": 0.3625,
      "step": 24900
    },
    {
      "epoch": 4.91,
      "grad_norm": 4.098160743713379,
      "learning_rate": 1.603792385223013e-07,
      "loss": 0.3688,
      "step": 25000
    },
    {
      "epoch": 4.93,
      "grad_norm": 4.397771835327148,
      "learning_rate": 9.640790402831723e-08,
      "loss": 0.3792,
      "step": 25100
    },
    {
      "epoch": 4.95,
      "grad_norm": 2.822801113128662,
      "learning_rate": 4.862294019757352e-08,
      "loss": 0.3832,
      "step": 25200
    },
    {
      "epoch": 4.97,
      "grad_norm": 2.5535385608673096,
      "learning_rate": 1.7032089145441187e-08,
      "loss": 0.359,
      "step": 25300
    },
    {
      "epoch": 4.99,
      "grad_norm": 4.649831771850586,
      "learning_rate": 1.6404692186555714e-09,
      "loss": 0.3517,
      "step": 25400
    }
  ],
  "logging_steps": 100,
  "max_steps": 25445,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "total_flos": 6.08607627442348e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
