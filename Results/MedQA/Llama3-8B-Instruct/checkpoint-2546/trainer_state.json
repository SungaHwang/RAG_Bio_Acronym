{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 2546,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.08,
      "grad_norm": 2.5190436840057373,
      "learning_rate": 6.535947712418301e-06,
      "loss": 2.0081,
      "step": 100
    },
    {
      "epoch": 0.16,
      "grad_norm": 2.052461862564087,
      "learning_rate": 1.3071895424836602e-05,
      "loss": 1.5058,
      "step": 200
    },
    {
      "epoch": 0.24,
      "grad_norm": 1.6995384693145752,
      "learning_rate": 1.9607843137254903e-05,
      "loss": 1.3937,
      "step": 300
    },
    {
      "epoch": 0.31,
      "grad_norm": 1.9064440727233887,
      "learning_rate": 1.9991889981715696e-05,
      "loss": 1.3525,
      "step": 400
    },
    {
      "epoch": 0.39,
      "grad_norm": 1.8869792222976685,
      "learning_rate": 1.9965471460186872e-05,
      "loss": 1.3622,
      "step": 500
    },
    {
      "epoch": 0.47,
      "grad_norm": 2.1874027252197266,
      "learning_rate": 1.9920759883782087e-05,
      "loss": 1.3451,
      "step": 600
    },
    {
      "epoch": 0.55,
      "grad_norm": 1.974693775177002,
      "learning_rate": 1.9857837327024763e-05,
      "loss": 1.3211,
      "step": 700
    },
    {
      "epoch": 0.63,
      "grad_norm": 1.7520865201950073,
      "learning_rate": 1.9776819293309636e-05,
      "loss": 1.3033,
      "step": 800
    },
    {
      "epoch": 0.71,
      "grad_norm": 1.7239516973495483,
      "learning_rate": 1.9677854502879647e-05,
      "loss": 1.3188,
      "step": 900
    },
    {
      "epoch": 0.79,
      "grad_norm": 1.7466368675231934,
      "learning_rate": 1.95611246198286e-05,
      "loss": 1.3066,
      "step": 1000
    },
    {
      "epoch": 0.86,
      "grad_norm": 1.7262258529663086,
      "learning_rate": 1.9426843918630605e-05,
      "loss": 1.3113,
      "step": 1100
    },
    {
      "epoch": 0.94,
      "grad_norm": 1.9320111274719238,
      "learning_rate": 1.9275258890808474e-05,
      "loss": 1.3115,
      "step": 1200
    },
    {
      "epoch": 1.02,
      "grad_norm": 1.2800401449203491,
      "learning_rate": 1.9106647792463117e-05,
      "loss": 1.2641,
      "step": 1300
    },
    {
      "epoch": 1.1,
      "grad_norm": 1.5594803094863892,
      "learning_rate": 1.892132013349451e-05,
      "loss": 1.2415,
      "step": 1400
    },
    {
      "epoch": 1.18,
      "grad_norm": 1.5076805353164673,
      "learning_rate": 1.8719616109451754e-05,
      "loss": 1.2484,
      "step": 1500
    },
    {
      "epoch": 1.26,
      "grad_norm": 1.5179493427276611,
      "learning_rate": 1.8501905977055297e-05,
      "loss": 1.2487,
      "step": 1600
    },
    {
      "epoch": 1.34,
      "grad_norm": 1.5097883939743042,
      "learning_rate": 1.8268589374537498e-05,
      "loss": 1.2519,
      "step": 1700
    },
    {
      "epoch": 1.41,
      "grad_norm": 1.5118768215179443,
      "learning_rate": 1.802009458804923e-05,
      "loss": 1.2448,
      "step": 1800
    },
    {
      "epoch": 1.49,
      "grad_norm": 1.6128805875778198,
      "learning_rate": 1.7756877765479107e-05,
      "loss": 1.251,
      "step": 1900
    },
    {
      "epoch": 1.57,
      "grad_norm": 1.6240652799606323,
      "learning_rate": 1.747942207912849e-05,
      "loss": 1.2272,
      "step": 2000
    },
    {
      "epoch": 1.65,
      "grad_norm": 1.5963724851608276,
      "learning_rate": 1.7188236838779297e-05,
      "loss": 1.2348,
      "step": 2100
    },
    {
      "epoch": 1.73,
      "grad_norm": 1.6804008483886719,
      "learning_rate": 1.6883856556782697e-05,
      "loss": 1.2295,
      "step": 2200
    },
    {
      "epoch": 1.81,
      "grad_norm": 1.7794355154037476,
      "learning_rate": 1.6566839966884906e-05,
      "loss": 1.2596,
      "step": 2300
    },
    {
      "epoch": 1.89,
      "grad_norm": 1.5247714519500732,
      "learning_rate": 1.6237768998591097e-05,
      "loss": 1.2374,
      "step": 2400
    },
    {
      "epoch": 1.96,
      "grad_norm": 1.6394332647323608,
      "learning_rate": 1.5897247708950157e-05,
      "loss": 1.2325,
      "step": 2500
    }
  ],
  "logging_steps": 100,
  "max_steps": 7638,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 6,
  "save_steps": 500,
  "total_flos": 2.0957998792138752e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
