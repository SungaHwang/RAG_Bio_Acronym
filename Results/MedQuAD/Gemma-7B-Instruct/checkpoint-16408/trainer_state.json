{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 16408,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02437835202340322,
      "grad_norm": 7.885652542114258,
      "learning_rate": 0.00019878108239882984,
      "loss": 3.8494,
      "step": 100
    },
    {
      "epoch": 0.04875670404680644,
      "grad_norm": 4.9701151847839355,
      "learning_rate": 0.00019756216479765968,
      "loss": 1.9399,
      "step": 200
    },
    {
      "epoch": 0.07313505607020965,
      "grad_norm": 6.60969352722168,
      "learning_rate": 0.00019634324719648954,
      "loss": 1.8187,
      "step": 300
    },
    {
      "epoch": 0.09751340809361288,
      "grad_norm": 3.8199644088745117,
      "learning_rate": 0.00019512432959531938,
      "loss": 1.7245,
      "step": 400
    },
    {
      "epoch": 0.12189176011701609,
      "grad_norm": 4.725223541259766,
      "learning_rate": 0.0001939054119941492,
      "loss": 1.7618,
      "step": 500
    },
    {
      "epoch": 0.1462701121404193,
      "grad_norm": 6.713329315185547,
      "learning_rate": 0.00019268649439297902,
      "loss": 1.7492,
      "step": 600
    },
    {
      "epoch": 0.17064846416382254,
      "grad_norm": 3.8568661212921143,
      "learning_rate": 0.00019146757679180888,
      "loss": 1.734,
      "step": 700
    },
    {
      "epoch": 0.19502681618722575,
      "grad_norm": 7.023652076721191,
      "learning_rate": 0.00019024865919063872,
      "loss": 1.7069,
      "step": 800
    },
    {
      "epoch": 0.21940516821062897,
      "grad_norm": 5.16474723815918,
      "learning_rate": 0.00018902974158946855,
      "loss": 1.733,
      "step": 900
    },
    {
      "epoch": 0.24378352023403219,
      "grad_norm": 5.315758228302002,
      "learning_rate": 0.0001878108239882984,
      "loss": 1.6391,
      "step": 1000
    },
    {
      "epoch": 0.2681618722574354,
      "grad_norm": 5.321823596954346,
      "learning_rate": 0.00018659190638712825,
      "loss": 1.6347,
      "step": 1100
    },
    {
      "epoch": 0.2925402242808386,
      "grad_norm": 9.205940246582031,
      "learning_rate": 0.00018537298878595809,
      "loss": 1.5981,
      "step": 1200
    },
    {
      "epoch": 0.3169185763042418,
      "grad_norm": 4.301296710968018,
      "learning_rate": 0.00018415407118478792,
      "loss": 1.6761,
      "step": 1300
    },
    {
      "epoch": 0.3412969283276451,
      "grad_norm": 6.222165584564209,
      "learning_rate": 0.00018293515358361776,
      "loss": 1.622,
      "step": 1400
    },
    {
      "epoch": 0.3656752803510483,
      "grad_norm": 3.437169075012207,
      "learning_rate": 0.0001817162359824476,
      "loss": 1.5368,
      "step": 1500
    },
    {
      "epoch": 0.3900536323744515,
      "grad_norm": 5.79552698135376,
      "learning_rate": 0.00018049731838127743,
      "loss": 1.6007,
      "step": 1600
    },
    {
      "epoch": 0.4144319843978547,
      "grad_norm": 4.184427738189697,
      "learning_rate": 0.00017927840078010726,
      "loss": 1.5548,
      "step": 1700
    },
    {
      "epoch": 0.43881033642125794,
      "grad_norm": 5.03712272644043,
      "learning_rate": 0.0001780594831789371,
      "loss": 1.5969,
      "step": 1800
    },
    {
      "epoch": 0.46318868844466116,
      "grad_norm": 7.73659610748291,
      "learning_rate": 0.00017684056557776696,
      "loss": 1.5503,
      "step": 1900
    },
    {
      "epoch": 0.48756704046806437,
      "grad_norm": 4.098001480102539,
      "learning_rate": 0.0001756216479765968,
      "loss": 1.5556,
      "step": 2000
    },
    {
      "epoch": 0.5119453924914675,
      "grad_norm": 5.721057415008545,
      "learning_rate": 0.00017440273037542663,
      "loss": 1.5145,
      "step": 2100
    },
    {
      "epoch": 0.5363237445148707,
      "grad_norm": 5.561791896820068,
      "learning_rate": 0.00017318381277425647,
      "loss": 1.5473,
      "step": 2200
    },
    {
      "epoch": 0.560702096538274,
      "grad_norm": 4.6710920333862305,
      "learning_rate": 0.00017196489517308633,
      "loss": 1.5571,
      "step": 2300
    },
    {
      "epoch": 0.5850804485616772,
      "grad_norm": 4.553824424743652,
      "learning_rate": 0.00017074597757191614,
      "loss": 1.4729,
      "step": 2400
    },
    {
      "epoch": 0.6094588005850804,
      "grad_norm": 6.198246479034424,
      "learning_rate": 0.00016952705997074597,
      "loss": 1.4698,
      "step": 2500
    },
    {
      "epoch": 0.6338371526084836,
      "grad_norm": 4.777071952819824,
      "learning_rate": 0.0001683081423695758,
      "loss": 1.4794,
      "step": 2600
    },
    {
      "epoch": 0.6582155046318869,
      "grad_norm": 4.112995624542236,
      "learning_rate": 0.00016708922476840567,
      "loss": 1.4211,
      "step": 2700
    },
    {
      "epoch": 0.6825938566552902,
      "grad_norm": 4.350918769836426,
      "learning_rate": 0.0001658703071672355,
      "loss": 1.539,
      "step": 2800
    },
    {
      "epoch": 0.7069722086786934,
      "grad_norm": 5.047833442687988,
      "learning_rate": 0.00016465138956606534,
      "loss": 1.479,
      "step": 2900
    },
    {
      "epoch": 0.7313505607020966,
      "grad_norm": 4.232733726501465,
      "learning_rate": 0.00016343247196489517,
      "loss": 1.4241,
      "step": 3000
    },
    {
      "epoch": 0.7557289127254998,
      "grad_norm": 4.94191312789917,
      "learning_rate": 0.00016221355436372504,
      "loss": 1.491,
      "step": 3100
    },
    {
      "epoch": 0.780107264748903,
      "grad_norm": 2.995272636413574,
      "learning_rate": 0.00016099463676255487,
      "loss": 1.4512,
      "step": 3200
    },
    {
      "epoch": 0.8044856167723062,
      "grad_norm": 4.9415483474731445,
      "learning_rate": 0.0001597757191613847,
      "loss": 1.4395,
      "step": 3300
    },
    {
      "epoch": 0.8288639687957094,
      "grad_norm": 5.740908145904541,
      "learning_rate": 0.00015855680156021454,
      "loss": 1.3754,
      "step": 3400
    },
    {
      "epoch": 0.8532423208191127,
      "grad_norm": 4.836285591125488,
      "learning_rate": 0.00015733788395904438,
      "loss": 1.4224,
      "step": 3500
    },
    {
      "epoch": 0.8776206728425159,
      "grad_norm": 5.876269817352295,
      "learning_rate": 0.0001561189663578742,
      "loss": 1.4458,
      "step": 3600
    },
    {
      "epoch": 0.9019990248659191,
      "grad_norm": 8.347146034240723,
      "learning_rate": 0.00015490004875670405,
      "loss": 1.4329,
      "step": 3700
    },
    {
      "epoch": 0.9263773768893223,
      "grad_norm": 5.609083652496338,
      "learning_rate": 0.00015368113115553388,
      "loss": 1.3999,
      "step": 3800
    },
    {
      "epoch": 0.9507557289127255,
      "grad_norm": 3.812046527862549,
      "learning_rate": 0.00015246221355436375,
      "loss": 1.3732,
      "step": 3900
    },
    {
      "epoch": 0.9751340809361287,
      "grad_norm": 6.698141098022461,
      "learning_rate": 0.00015124329595319358,
      "loss": 1.4023,
      "step": 4000
    },
    {
      "epoch": 0.999512432959532,
      "grad_norm": 6.031205654144287,
      "learning_rate": 0.00015002437835202342,
      "loss": 1.4265,
      "step": 4100
    },
    {
      "epoch": 1.023890784982935,
      "grad_norm": 5.373346328735352,
      "learning_rate": 0.00014880546075085325,
      "loss": 1.2663,
      "step": 4200
    },
    {
      "epoch": 1.0482691370063384,
      "grad_norm": 5.323412895202637,
      "learning_rate": 0.0001475865431496831,
      "loss": 1.2671,
      "step": 4300
    },
    {
      "epoch": 1.0726474890297415,
      "grad_norm": 3.769141435623169,
      "learning_rate": 0.00014636762554851292,
      "loss": 1.2763,
      "step": 4400
    },
    {
      "epoch": 1.0970258410531448,
      "grad_norm": 3.9969117641448975,
      "learning_rate": 0.00014514870794734276,
      "loss": 1.2851,
      "step": 4500
    },
    {
      "epoch": 1.121404193076548,
      "grad_norm": 5.019802570343018,
      "learning_rate": 0.0001439297903461726,
      "loss": 1.2479,
      "step": 4600
    },
    {
      "epoch": 1.1457825450999513,
      "grad_norm": 3.313371419906616,
      "learning_rate": 0.00014271087274500245,
      "loss": 1.3012,
      "step": 4700
    },
    {
      "epoch": 1.1701608971233544,
      "grad_norm": 3.711179494857788,
      "learning_rate": 0.0001414919551438323,
      "loss": 1.2938,
      "step": 4800
    },
    {
      "epoch": 1.1945392491467577,
      "grad_norm": 5.058886528015137,
      "learning_rate": 0.00014027303754266213,
      "loss": 1.2875,
      "step": 4900
    },
    {
      "epoch": 1.218917601170161,
      "grad_norm": 3.194885492324829,
      "learning_rate": 0.00013905411994149196,
      "loss": 1.3066,
      "step": 5000
    },
    {
      "epoch": 1.2432959531935641,
      "grad_norm": 3.4778096675872803,
      "learning_rate": 0.00013783520234032182,
      "loss": 1.302,
      "step": 5100
    },
    {
      "epoch": 1.2676743052169672,
      "grad_norm": 3.4593565464019775,
      "learning_rate": 0.00013661628473915166,
      "loss": 1.273,
      "step": 5200
    },
    {
      "epoch": 1.2920526572403706,
      "grad_norm": 5.913288116455078,
      "learning_rate": 0.00013539736713798147,
      "loss": 1.2681,
      "step": 5300
    },
    {
      "epoch": 1.3164310092637739,
      "grad_norm": 2.909898281097412,
      "learning_rate": 0.0001341784495368113,
      "loss": 1.2946,
      "step": 5400
    },
    {
      "epoch": 1.340809361287177,
      "grad_norm": 4.256062984466553,
      "learning_rate": 0.00013295953193564114,
      "loss": 1.2958,
      "step": 5500
    },
    {
      "epoch": 1.36518771331058,
      "grad_norm": 4.663949012756348,
      "learning_rate": 0.000131740614334471,
      "loss": 1.2755,
      "step": 5600
    },
    {
      "epoch": 1.3895660653339834,
      "grad_norm": 3.5128746032714844,
      "learning_rate": 0.00013052169673330083,
      "loss": 1.2545,
      "step": 5700
    },
    {
      "epoch": 1.4139444173573867,
      "grad_norm": 4.2748212814331055,
      "learning_rate": 0.00012930277913213067,
      "loss": 1.2798,
      "step": 5800
    },
    {
      "epoch": 1.4383227693807898,
      "grad_norm": 3.5787172317504883,
      "learning_rate": 0.0001280838615309605,
      "loss": 1.2733,
      "step": 5900
    },
    {
      "epoch": 1.462701121404193,
      "grad_norm": 3.068992853164673,
      "learning_rate": 0.00012686494392979037,
      "loss": 1.3048,
      "step": 6000
    },
    {
      "epoch": 1.4870794734275963,
      "grad_norm": 3.078623056411743,
      "learning_rate": 0.0001256460263286202,
      "loss": 1.2804,
      "step": 6100
    },
    {
      "epoch": 1.5114578254509996,
      "grad_norm": 3.7018163204193115,
      "learning_rate": 0.00012442710872745004,
      "loss": 1.2827,
      "step": 6200
    },
    {
      "epoch": 1.5358361774744027,
      "grad_norm": 6.445744037628174,
      "learning_rate": 0.00012320819112627987,
      "loss": 1.281,
      "step": 6300
    },
    {
      "epoch": 1.5602145294978058,
      "grad_norm": 3.211289405822754,
      "learning_rate": 0.00012198927352510972,
      "loss": 1.2251,
      "step": 6400
    },
    {
      "epoch": 1.5845928815212091,
      "grad_norm": 3.6751837730407715,
      "learning_rate": 0.00012077035592393954,
      "loss": 1.2641,
      "step": 6500
    },
    {
      "epoch": 1.6089712335446125,
      "grad_norm": 3.9917092323303223,
      "learning_rate": 0.00011955143832276938,
      "loss": 1.2205,
      "step": 6600
    },
    {
      "epoch": 1.6333495855680156,
      "grad_norm": 3.3570690155029297,
      "learning_rate": 0.00011833252072159921,
      "loss": 1.2642,
      "step": 6700
    },
    {
      "epoch": 1.6577279375914187,
      "grad_norm": 3.8118550777435303,
      "learning_rate": 0.00011711360312042908,
      "loss": 1.242,
      "step": 6800
    },
    {
      "epoch": 1.682106289614822,
      "grad_norm": 3.696451425552368,
      "learning_rate": 0.00011589468551925891,
      "loss": 1.2869,
      "step": 6900
    },
    {
      "epoch": 1.7064846416382253,
      "grad_norm": 4.646883487701416,
      "learning_rate": 0.00011467576791808873,
      "loss": 1.2446,
      "step": 7000
    },
    {
      "epoch": 1.7308629936616284,
      "grad_norm": 3.6454312801361084,
      "learning_rate": 0.00011345685031691857,
      "loss": 1.228,
      "step": 7100
    },
    {
      "epoch": 1.7552413456850315,
      "grad_norm": 4.400160789489746,
      "learning_rate": 0.00011223793271574843,
      "loss": 1.2099,
      "step": 7200
    },
    {
      "epoch": 1.7796196977084349,
      "grad_norm": 3.673815965652466,
      "learning_rate": 0.00011101901511457827,
      "loss": 1.2626,
      "step": 7300
    },
    {
      "epoch": 1.8039980497318382,
      "grad_norm": 4.174151420593262,
      "learning_rate": 0.0001098000975134081,
      "loss": 1.2578,
      "step": 7400
    },
    {
      "epoch": 1.8283764017552413,
      "grad_norm": 4.349677562713623,
      "learning_rate": 0.00010858117991223794,
      "loss": 1.203,
      "step": 7500
    },
    {
      "epoch": 1.8527547537786444,
      "grad_norm": 4.2496113777160645,
      "learning_rate": 0.00010736226231106778,
      "loss": 1.2148,
      "step": 7600
    },
    {
      "epoch": 1.8771331058020477,
      "grad_norm": 4.308843612670898,
      "learning_rate": 0.00010614334470989762,
      "loss": 1.2079,
      "step": 7700
    },
    {
      "epoch": 1.901511457825451,
      "grad_norm": 4.053042888641357,
      "learning_rate": 0.00010492442710872745,
      "loss": 1.2456,
      "step": 7800
    },
    {
      "epoch": 1.9258898098488544,
      "grad_norm": 3.298527240753174,
      "learning_rate": 0.00010370550950755729,
      "loss": 1.2039,
      "step": 7900
    },
    {
      "epoch": 1.9502681618722575,
      "grad_norm": 3.446326494216919,
      "learning_rate": 0.00010248659190638714,
      "loss": 1.2388,
      "step": 8000
    },
    {
      "epoch": 1.9746465138956606,
      "grad_norm": 3.4068291187286377,
      "learning_rate": 0.00010126767430521697,
      "loss": 1.2647,
      "step": 8100
    },
    {
      "epoch": 1.999024865919064,
      "grad_norm": 3.5539474487304688,
      "learning_rate": 0.00010004875670404681,
      "loss": 1.2167,
      "step": 8200
    },
    {
      "epoch": 2.0234032179424672,
      "grad_norm": 3.17282772064209,
      "learning_rate": 9.882983910287666e-05,
      "loss": 1.1308,
      "step": 8300
    },
    {
      "epoch": 2.04778156996587,
      "grad_norm": 3.4378480911254883,
      "learning_rate": 9.761092150170649e-05,
      "loss": 1.1213,
      "step": 8400
    },
    {
      "epoch": 2.0721599219892735,
      "grad_norm": 3.4578065872192383,
      "learning_rate": 9.639200390053633e-05,
      "loss": 1.1513,
      "step": 8500
    },
    {
      "epoch": 2.096538274012677,
      "grad_norm": 3.1392858028411865,
      "learning_rate": 9.517308629936616e-05,
      "loss": 1.1527,
      "step": 8600
    },
    {
      "epoch": 2.12091662603608,
      "grad_norm": 2.7517433166503906,
      "learning_rate": 9.395416869819601e-05,
      "loss": 1.146,
      "step": 8700
    },
    {
      "epoch": 2.145294978059483,
      "grad_norm": 3.705005168914795,
      "learning_rate": 9.273525109702585e-05,
      "loss": 1.1314,
      "step": 8800
    },
    {
      "epoch": 2.1696733300828863,
      "grad_norm": 3.5301876068115234,
      "learning_rate": 9.151633349585568e-05,
      "loss": 1.1599,
      "step": 8900
    },
    {
      "epoch": 2.1940516821062896,
      "grad_norm": 3.135011911392212,
      "learning_rate": 9.029741589468552e-05,
      "loss": 1.1179,
      "step": 9000
    },
    {
      "epoch": 2.218430034129693,
      "grad_norm": 2.727281093597412,
      "learning_rate": 8.907849829351537e-05,
      "loss": 1.1521,
      "step": 9100
    },
    {
      "epoch": 2.242808386153096,
      "grad_norm": 3.8083579540252686,
      "learning_rate": 8.78595806923452e-05,
      "loss": 1.148,
      "step": 9200
    },
    {
      "epoch": 2.267186738176499,
      "grad_norm": 3.929034471511841,
      "learning_rate": 8.664066309117505e-05,
      "loss": 1.1193,
      "step": 9300
    },
    {
      "epoch": 2.2915650901999025,
      "grad_norm": 3.29860782623291,
      "learning_rate": 8.542174549000487e-05,
      "loss": 1.152,
      "step": 9400
    },
    {
      "epoch": 2.315943442223306,
      "grad_norm": 3.5222079753875732,
      "learning_rate": 8.420282788883472e-05,
      "loss": 1.1204,
      "step": 9500
    },
    {
      "epoch": 2.3403217942467087,
      "grad_norm": 3.567032814025879,
      "learning_rate": 8.298391028766456e-05,
      "loss": 1.1594,
      "step": 9600
    },
    {
      "epoch": 2.364700146270112,
      "grad_norm": 3.021608352661133,
      "learning_rate": 8.17649926864944e-05,
      "loss": 1.148,
      "step": 9700
    },
    {
      "epoch": 2.3890784982935154,
      "grad_norm": 2.426676034927368,
      "learning_rate": 8.054607508532424e-05,
      "loss": 1.1425,
      "step": 9800
    },
    {
      "epoch": 2.4134568503169187,
      "grad_norm": 3.851733684539795,
      "learning_rate": 7.932715748415408e-05,
      "loss": 1.1323,
      "step": 9900
    },
    {
      "epoch": 2.437835202340322,
      "grad_norm": 3.0911803245544434,
      "learning_rate": 7.810823988298391e-05,
      "loss": 1.1039,
      "step": 10000
    },
    {
      "epoch": 2.462213554363725,
      "grad_norm": 2.7414073944091797,
      "learning_rate": 7.688932228181376e-05,
      "loss": 1.1464,
      "step": 10100
    },
    {
      "epoch": 2.4865919063871282,
      "grad_norm": 2.237525224685669,
      "learning_rate": 7.56704046806436e-05,
      "loss": 1.1622,
      "step": 10200
    },
    {
      "epoch": 2.5109702584105316,
      "grad_norm": 2.5186314582824707,
      "learning_rate": 7.445148707947343e-05,
      "loss": 1.1279,
      "step": 10300
    },
    {
      "epoch": 2.5353486104339344,
      "grad_norm": 3.086333990097046,
      "learning_rate": 7.323256947830327e-05,
      "loss": 1.0913,
      "step": 10400
    },
    {
      "epoch": 2.5597269624573378,
      "grad_norm": 3.615556001663208,
      "learning_rate": 7.201365187713311e-05,
      "loss": 1.1456,
      "step": 10500
    },
    {
      "epoch": 2.584105314480741,
      "grad_norm": 3.80461049079895,
      "learning_rate": 7.079473427596295e-05,
      "loss": 1.1328,
      "step": 10600
    },
    {
      "epoch": 2.6084836665041444,
      "grad_norm": 4.642910957336426,
      "learning_rate": 6.957581667479278e-05,
      "loss": 1.1265,
      "step": 10700
    },
    {
      "epoch": 2.6328620185275478,
      "grad_norm": 4.85848331451416,
      "learning_rate": 6.835689907362263e-05,
      "loss": 1.135,
      "step": 10800
    },
    {
      "epoch": 2.6572403705509506,
      "grad_norm": 2.4407622814178467,
      "learning_rate": 6.713798147245245e-05,
      "loss": 1.1453,
      "step": 10900
    },
    {
      "epoch": 2.681618722574354,
      "grad_norm": 3.8627970218658447,
      "learning_rate": 6.59190638712823e-05,
      "loss": 1.126,
      "step": 11000
    },
    {
      "epoch": 2.7059970745977573,
      "grad_norm": 3.567326545715332,
      "learning_rate": 6.470014627011214e-05,
      "loss": 1.1262,
      "step": 11100
    },
    {
      "epoch": 2.73037542662116,
      "grad_norm": 3.4333465099334717,
      "learning_rate": 6.348122866894199e-05,
      "loss": 1.1284,
      "step": 11200
    },
    {
      "epoch": 2.7547537786445635,
      "grad_norm": 3.488210678100586,
      "learning_rate": 6.226231106777182e-05,
      "loss": 1.1627,
      "step": 11300
    },
    {
      "epoch": 2.779132130667967,
      "grad_norm": 4.624207496643066,
      "learning_rate": 6.104339346660166e-05,
      "loss": 1.1235,
      "step": 11400
    },
    {
      "epoch": 2.80351048269137,
      "grad_norm": 3.6964120864868164,
      "learning_rate": 5.9824475865431493e-05,
      "loss": 1.1243,
      "step": 11500
    },
    {
      "epoch": 2.8278888347147735,
      "grad_norm": 2.478370428085327,
      "learning_rate": 5.860555826426134e-05,
      "loss": 1.1222,
      "step": 11600
    },
    {
      "epoch": 2.8522671867381764,
      "grad_norm": 2.2284202575683594,
      "learning_rate": 5.738664066309117e-05,
      "loss": 1.1109,
      "step": 11700
    },
    {
      "epoch": 2.8766455387615797,
      "grad_norm": 2.3422152996063232,
      "learning_rate": 5.616772306192102e-05,
      "loss": 1.1098,
      "step": 11800
    },
    {
      "epoch": 2.901023890784983,
      "grad_norm": 3.6212871074676514,
      "learning_rate": 5.4948805460750855e-05,
      "loss": 1.1071,
      "step": 11900
    },
    {
      "epoch": 2.925402242808386,
      "grad_norm": 2.719298839569092,
      "learning_rate": 5.3729887859580697e-05,
      "loss": 1.1182,
      "step": 12000
    },
    {
      "epoch": 2.9497805948317892,
      "grad_norm": 2.5357320308685303,
      "learning_rate": 5.251097025841053e-05,
      "loss": 1.0834,
      "step": 12100
    },
    {
      "epoch": 2.9741589468551926,
      "grad_norm": 3.2654757499694824,
      "learning_rate": 5.1292052657240374e-05,
      "loss": 1.0757,
      "step": 12200
    },
    {
      "epoch": 2.998537298878596,
      "grad_norm": 3.591684341430664,
      "learning_rate": 5.007313505607021e-05,
      "loss": 1.1187,
      "step": 12300
    },
    {
      "epoch": 3.022915650901999,
      "grad_norm": 2.1305227279663086,
      "learning_rate": 4.885421745490005e-05,
      "loss": 1.0518,
      "step": 12400
    },
    {
      "epoch": 3.047294002925402,
      "grad_norm": 2.0448005199432373,
      "learning_rate": 4.763529985372989e-05,
      "loss": 1.0148,
      "step": 12500
    },
    {
      "epoch": 3.0716723549488054,
      "grad_norm": 1.940615177154541,
      "learning_rate": 4.641638225255973e-05,
      "loss": 1.0213,
      "step": 12600
    },
    {
      "epoch": 3.0960507069722087,
      "grad_norm": 2.3651134967803955,
      "learning_rate": 4.519746465138957e-05,
      "loss": 1.0402,
      "step": 12700
    },
    {
      "epoch": 3.120429058995612,
      "grad_norm": 2.5746099948883057,
      "learning_rate": 4.397854705021941e-05,
      "loss": 1.0464,
      "step": 12800
    },
    {
      "epoch": 3.144807411019015,
      "grad_norm": 2.339702606201172,
      "learning_rate": 4.275962944904925e-05,
      "loss": 1.0442,
      "step": 12900
    },
    {
      "epoch": 3.1691857630424183,
      "grad_norm": 2.0218505859375,
      "learning_rate": 4.154071184787909e-05,
      "loss": 1.0557,
      "step": 13000
    },
    {
      "epoch": 3.1935641150658216,
      "grad_norm": 2.896141529083252,
      "learning_rate": 4.0321794246708925e-05,
      "loss": 1.048,
      "step": 13100
    },
    {
      "epoch": 3.217942467089225,
      "grad_norm": 2.79109787940979,
      "learning_rate": 3.910287664553876e-05,
      "loss": 1.0514,
      "step": 13200
    },
    {
      "epoch": 3.242320819112628,
      "grad_norm": 2.6454663276672363,
      "learning_rate": 3.78839590443686e-05,
      "loss": 1.0561,
      "step": 13300
    },
    {
      "epoch": 3.266699171136031,
      "grad_norm": 2.3306806087493896,
      "learning_rate": 3.666504144319844e-05,
      "loss": 1.0164,
      "step": 13400
    },
    {
      "epoch": 3.2910775231594345,
      "grad_norm": 1.9001848697662354,
      "learning_rate": 3.544612384202828e-05,
      "loss": 1.0411,
      "step": 13500
    },
    {
      "epoch": 3.315455875182838,
      "grad_norm": 2.5268619060516357,
      "learning_rate": 3.4227206240858114e-05,
      "loss": 1.0369,
      "step": 13600
    },
    {
      "epoch": 3.3398342272062407,
      "grad_norm": 2.2977828979492188,
      "learning_rate": 3.3008288639687956e-05,
      "loss": 1.0571,
      "step": 13700
    },
    {
      "epoch": 3.364212579229644,
      "grad_norm": 3.7339823246002197,
      "learning_rate": 3.17893710385178e-05,
      "loss": 1.0215,
      "step": 13800
    },
    {
      "epoch": 3.3885909312530473,
      "grad_norm": 3.263871669769287,
      "learning_rate": 3.0570453437347634e-05,
      "loss": 1.0439,
      "step": 13900
    },
    {
      "epoch": 3.4129692832764507,
      "grad_norm": 2.644906997680664,
      "learning_rate": 2.9351535836177476e-05,
      "loss": 1.04,
      "step": 14000
    },
    {
      "epoch": 3.4373476352998535,
      "grad_norm": 3.363100290298462,
      "learning_rate": 2.8132618235007314e-05,
      "loss": 1.0686,
      "step": 14100
    },
    {
      "epoch": 3.461725987323257,
      "grad_norm": 2.6220033168792725,
      "learning_rate": 2.6913700633837153e-05,
      "loss": 1.0202,
      "step": 14200
    },
    {
      "epoch": 3.48610433934666,
      "grad_norm": 3.0664830207824707,
      "learning_rate": 2.569478303266699e-05,
      "loss": 1.0656,
      "step": 14300
    },
    {
      "epoch": 3.5104826913700635,
      "grad_norm": 3.0293688774108887,
      "learning_rate": 2.4475865431496833e-05,
      "loss": 1.0551,
      "step": 14400
    },
    {
      "epoch": 3.534861043393467,
      "grad_norm": 2.73417329788208,
      "learning_rate": 2.325694783032667e-05,
      "loss": 1.0244,
      "step": 14500
    },
    {
      "epoch": 3.5592393954168697,
      "grad_norm": 3.228242874145508,
      "learning_rate": 2.2038030229156507e-05,
      "loss": 1.0159,
      "step": 14600
    },
    {
      "epoch": 3.583617747440273,
      "grad_norm": 2.860476493835449,
      "learning_rate": 2.081911262798635e-05,
      "loss": 1.0393,
      "step": 14700
    },
    {
      "epoch": 3.6079960994636764,
      "grad_norm": 3.4374446868896484,
      "learning_rate": 1.9600195026816188e-05,
      "loss": 1.0286,
      "step": 14800
    },
    {
      "epoch": 3.6323744514870793,
      "grad_norm": 2.8383264541625977,
      "learning_rate": 1.8381277425646026e-05,
      "loss": 1.018,
      "step": 14900
    },
    {
      "epoch": 3.6567528035104826,
      "grad_norm": 2.0369455814361572,
      "learning_rate": 1.7162359824475865e-05,
      "loss": 1.0606,
      "step": 15000
    },
    {
      "epoch": 3.681131155533886,
      "grad_norm": 2.7587013244628906,
      "learning_rate": 1.5943442223305704e-05,
      "loss": 1.0413,
      "step": 15100
    },
    {
      "epoch": 3.7055095075572893,
      "grad_norm": 2.7611801624298096,
      "learning_rate": 1.4724524622135544e-05,
      "loss": 1.0498,
      "step": 15200
    },
    {
      "epoch": 3.7298878595806926,
      "grad_norm": 2.9003474712371826,
      "learning_rate": 1.3505607020965382e-05,
      "loss": 1.0306,
      "step": 15300
    },
    {
      "epoch": 3.7542662116040955,
      "grad_norm": 2.6395108699798584,
      "learning_rate": 1.2286689419795223e-05,
      "loss": 1.0459,
      "step": 15400
    },
    {
      "epoch": 3.778644563627499,
      "grad_norm": 2.919121265411377,
      "learning_rate": 1.1067771818625061e-05,
      "loss": 1.0191,
      "step": 15500
    },
    {
      "epoch": 3.803022915650902,
      "grad_norm": 3.3812952041625977,
      "learning_rate": 9.8488542174549e-06,
      "loss": 1.0314,
      "step": 15600
    },
    {
      "epoch": 3.827401267674305,
      "grad_norm": 2.3915321826934814,
      "learning_rate": 8.62993661628474e-06,
      "loss": 1.035,
      "step": 15700
    },
    {
      "epoch": 3.8517796196977083,
      "grad_norm": 2.311338186264038,
      "learning_rate": 7.411019015114579e-06,
      "loss": 1.0184,
      "step": 15800
    },
    {
      "epoch": 3.8761579717211117,
      "grad_norm": 1.9898420572280884,
      "learning_rate": 6.1921014139444175e-06,
      "loss": 1.0406,
      "step": 15900
    },
    {
      "epoch": 3.900536323744515,
      "grad_norm": 3.3615612983703613,
      "learning_rate": 4.973183812774257e-06,
      "loss": 1.0172,
      "step": 16000
    },
    {
      "epoch": 3.9249146757679183,
      "grad_norm": 1.9888615608215332,
      "learning_rate": 3.754266211604096e-06,
      "loss": 1.0223,
      "step": 16100
    },
    {
      "epoch": 3.949293027791321,
      "grad_norm": 2.36540150642395,
      "learning_rate": 2.535348610433935e-06,
      "loss": 1.0329,
      "step": 16200
    },
    {
      "epoch": 3.9736713798147245,
      "grad_norm": 1.8921059370040894,
      "learning_rate": 1.3164310092637738e-06,
      "loss": 1.0224,
      "step": 16300
    },
    {
      "epoch": 3.998049731838128,
      "grad_norm": 2.1348373889923096,
      "learning_rate": 9.751340809361287e-08,
      "loss": 1.036,
      "step": 16400
    }
  ],
  "logging_steps": 100,
  "max_steps": 16408,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.2435900066816e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
